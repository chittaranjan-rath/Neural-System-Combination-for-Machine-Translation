{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "Project_NMT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sY5mSUvX1xGc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "debug = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXUDm2s85ZNZ",
        "colab_type": "code",
        "outputId": "ef1c428b-b631-494d-fbe0-362726377e94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CXKVJL21xGu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwEmFj511xHA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "UNK_token = 2\n",
        "# MAX_LENGTH = 1000\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\", 2:\"UNK\"}\n",
        "        self.n_words = 3  # Count SOS and EOS and UNK\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQMRS-ktqCB6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s\n",
        "\n",
        "def Reverse(lst): \n",
        "    return [ele for ele in reversed(lst)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78wxulOl1xHY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def readLangs(lang1, lang2, reverse=False, model=\"dev\"):\n",
        "    \n",
        "    if model == \"dev\":\n",
        "        source = \"/content/gdrive/My Drive/NLPA/NLA S20 - Assignment 2 Data/enghin/dev.en\"\n",
        "        target = \"/content/gdrive/My Drive/NLPA/NLA S20 - Assignment 2 Data/enghin/dev.hi\"\n",
        "    else:\n",
        "        source = \"/content/gdrive/My Drive/NLPA/NLA S20 - Assignment 2 Data/enghin/train.en\"\n",
        "        target = \"/content/gdrive/My Drive/NLPA/NLA S20 - Assignment 2 Data/enghin/train.hi\"\n",
        "    \n",
        "    eng_lines = open(source, encoding='utf-8').read().strip().split('\\n')\n",
        "    hin_lines = open(target, encoding='utf-8').read().strip().split('\\n')\n",
        "\n",
        "    lines = []\n",
        "    for i in range(len(eng_lines)):\n",
        "        lines.append(eng_lines[i] + '\\t' + hin_lines[i])\n",
        "\n",
        "    # Split every line into pairs and normalize\n",
        "    # pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "    pairs = [[s for s in l.split('\\t')] for l in lines]\n",
        "    #print(pairs)\n",
        "    # Reverse pairs, make Lang instances\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(lang2)\n",
        "        output_lang = Lang(lang1)\n",
        "    else:\n",
        "        input_lang = Lang(lang1)\n",
        "        output_lang = Lang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37LCeZbCCi9q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LENGTH = 20\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH \n",
        "\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPT1Pzc11xHq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepareData(lang1, lang2, reverse=False, model=\"dev\", filter_sentence=False):\n",
        "    \n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse, model)\n",
        "    #print(input_lang, output_lang, pairs)\n",
        "    if(filter_sentence):\n",
        "        pairs = filterPairs(pairs)\n",
        "    \n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    \n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    \n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    \n",
        "    return input_lang, output_lang, pairs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcRQAZf9DB4O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "c5fb2185-5b4b-4edc-ba78-9e058516c9b6"
      },
      "source": [
        "input_lang, output_lang, pairs = prepareData('eng', 'hi', reverse=False, model=\"dev\", filter_sentence=True)\n",
        "print(random.choice(pairs))\n",
        "orig_pairs = []\n",
        "for p in pairs:\n",
        "    temp = str(p[0]).split(' ')\n",
        "    temp = Reverse(temp)\n",
        "    listToStr = ' '.join([str(elem) for elem in temp]) \n",
        "#     print(\"listToStr\")\n",
        "#     print(listToStr)\n",
        "    listToStr = str(listToStr)\n",
        "    orig_pairs.append(listToStr+'\\t'+str(p[1]))\n",
        "#     print(orig_pairs[-1])\n",
        "print(\"orig_pairs\")\n",
        "print(orig_pairs[0])\n",
        "print(random.choice(orig_pairs))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Read 256 sentence pairs\n",
            "Trimmed to 256 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "eng 1117\n",
            "hi 1170\n",
            "['A person starts falling ill again and again and continuously goes on weakening .', 'व्यक्ति बार - बार बीमार पड़ने लगता है और निरन्तर कमजोर होता जाता है ।']\n",
            "orig_pairs\n",
            ". diseases eye from safe be also will you but\tबल्कि आप नेत्ररोगों से भी बचे रहेंगे ।\n",
            ". Darjeeling of mountains the among low very placed are places these Both\tये दोनों ही स्थान दार्जिलिंग की पहाड़ियों के बीच बहुत नीचे विराजमान हैं ।\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpSD5Bb_1xHz",
        "colab_type": "text"
      },
      "source": [
        "The Encoder\n",
        "-----------\n",
        "\n",
        "The encoder of a seq2seq network is a RNN that outputs some value for\n",
        "every word from the input sentence. For every input word the encoder\n",
        "outputs a vector and a hidden state, and uses the hidden state for the\n",
        "next input word.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDZgT8g_1xH1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        # self.gru = nn.GRU(self.hidden_size, hidden_size, batch_first=False, bidirectional=True)\n",
        "        self.gru = nn.GRU(self.hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XajZxxXr1xIF",
        "colab_type": "text"
      },
      "source": [
        "The Attention Decoder\n",
        "---------------------\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2HAV8oS1xIH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        # self.gru = nn.GRU(self.hidden_size, self.hidden_size, batch_first=False, bidirectional=True)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "        self.attn_coverage = nn.Linear(self.max_length, self.hidden_size)\n",
        "        self.attn_coverage_cat = nn.Linear(self.hidden_size*3, self.max_length)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "       \n",
        "        attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "\n",
        "        # Q4 Chnage\n",
        "        lcl_cumulative_sum = torch.cumsum(attn_weights, -1)\n",
        "        lcl_Y1 = self.attn_coverage(lcl_cumulative_sum)\n",
        "        lcl_Y2 = torch.cat((torch.cat((embedded[0], hidden[0]), 1), lcl_Y1),1)\n",
        "        attn_weights_lcl = self.attn_coverage_cat(lcl_Y2)\n",
        "        attn_weights = F.softmax(attn_weights_lcl, dim = 1)\n",
        "        # Q4 Chnage\n",
        "\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHlWxIcA1xIM",
        "colab_type": "text"
      },
      "source": [
        "Training\n",
        "========\n",
        "\n",
        "Preparing Training Data\n",
        "-----------------------\n",
        "\n",
        "To train, for each pair we will need an input tensor (indexes of the\n",
        "words in the input sentence) and target tensor (indexes of the words in\n",
        "the target sentence). While creating these vectors we will append the\n",
        "EOS token to both sequences.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUxlLxyS1xIR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] if word in lang.word2index else UNK_token for word in sentence.split(' ')]\n",
        "\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMNj3Uiq1xIX",
        "colab_type": "text"
      },
      "source": [
        "Training the Model\n",
        "------------------\n",
        "\n",
        "To train we run the input sentence through the encoder, and keep track\n",
        "of every output and the latest hidden state. Then the decoder is given\n",
        "the ``<SOS>`` token as its first input, and the last hidden state of the\n",
        "encoder as its first hidden state.\n",
        "\n",
        "\"Teacher forcing\" is the concept of using the real target outputs as\n",
        "each next input, instead of using the decoder's guess as the next input.\n",
        "Using teacher forcing causes it to converge faster but `when the trained\n",
        "network is exploited, it may exhibit\n",
        "instability <http://minds.jacobs-university.de/sites/default/files/uploads/papers/ESNTutorialRev.pdf>`__.\n",
        "\n",
        "You can observe outputs of teacher-forced networks that read with\n",
        "coherent grammar but wander far from the correct translation -\n",
        "intuitively it has learned to represent the output grammar and can \"pick\n",
        "up\" the meaning once the teacher tells it the first few words, but it\n",
        "has not properly learned how to create the sentence from the translation\n",
        "in the first place.\n",
        "\n",
        "Because of the freedom PyTorch's autograd gives us, we can randomly\n",
        "choose to use teacher forcing or not with a simple if statement. Turn\n",
        "``teacher_forcing_ratio`` up to use more of it.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLz4jGbX1xIY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            # For Paper 2 \n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            \n",
        "            \n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            # For Paper 2 \n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            \n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzusY2u01xId",
        "colab_type": "text"
      },
      "source": [
        "This is a helper function to print time elapsed and estimated time\n",
        "remaining given the current time and progress %.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNovrvZq1xIf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7_F87qsFeOc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_list = []\n",
        "epoch_list = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ib6gNJsH1xIk",
        "colab_type": "text"
      },
      "source": [
        "The whole training process looks like this:\n",
        "\n",
        "-  Start a timer\n",
        "-  Initialize optimizers and criterion\n",
        "-  Create set of training pairs\n",
        "-  Start empty losses array for plotting\n",
        "\n",
        "Then we call ``train`` many times and occasionally print the progress (%\n",
        "of examples, time so far, estimated time) and average loss.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PmF1Obu1xIl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
        "                      for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder,\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            \n",
        "            loss_list.append(print_loss_avg)\n",
        "            epoch_list.append(iter)\n",
        "            print('epoch = ',epoch_list[-1],'  loss = ',loss_list[-1])\n",
        "\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yrJp-_A1xIr",
        "colab_type": "text"
      },
      "source": [
        "Plotting results\n",
        "----------------\n",
        "\n",
        "Plotting is done with matplotlib, using the array of loss values\n",
        "``plot_losses`` saved while training.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zOTwahz1xIu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def showPlot(loss_list, epoch_list):\n",
        "    plt.plot(epoch_list, loss_list)\n",
        "    plt.xticks(np.arange(0, 75000, 10000)) \n",
        "    plt.yticks(np.arange(0, 5, 0.5)) \n",
        "    plt.savefig(\"test.png\")\n",
        "    plt.show()\n",
        "    plt.close('all')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxX1ngU71xI0",
        "colab_type": "text"
      },
      "source": [
        "Evaluation\n",
        "==========\n",
        "\n",
        "Evaluation is mostly the same as training, but there are no targets so\n",
        "we simply feed the decoder's predictions back to itself for each step.\n",
        "Every time it predicts a word we add it to the output string, and if it\n",
        "predicts the EOS token we stop there. We also store the decoder's\n",
        "attention outputs for display later.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQqS3Uzi1xI1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            try:\n",
        "              if topi.item() == UNK_token:\n",
        "                  decoded_words.append('<UNK>')\n",
        "              if topi.item() == EOS_token:\n",
        "                  decoded_words.append('<EOS>')\n",
        "                  break\n",
        "              else:\n",
        "                  decoded_words.append(output_lang.index2word[topi.item()])\n",
        "            except:\n",
        "              continue\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words, decoder_attentions[:di + 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhXYW1_81xI6",
        "colab_type": "text"
      },
      "source": [
        "We can evaluate random sentences from the training set and print out the\n",
        "input, target, and output to make some subjective quality judgements:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBupSgVq1xI8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(orig_pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_k83y18YF5Kd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 962
        },
        "outputId": "d424a722-d5aa-4b29-bb80-ba6cde314244"
      },
      "source": [
        "\n",
        "print(output_lang.n_words)\n",
        "print(input_lang.n_words)\n",
        "hidden_size = 256\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
        "\n",
        "trainIters(encoder1, attn_decoder1, 25000, print_every=1000)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1170\n",
            "1117\n",
            "epoch =  1000   loss =  4.617335979790813\n",
            "0m 34s (- 13m 54s) (1000 4%) 4.6173\n",
            "epoch =  2000   loss =  4.439340661458477\n",
            "1m 9s (- 13m 20s) (2000 8%) 4.4393\n",
            "epoch =  3000   loss =  3.812911378598869\n",
            "1m 46s (- 12m 57s) (3000 12%) 3.8129\n",
            "epoch =  4000   loss =  2.618166730167369\n",
            "2m 22s (- 12m 28s) (4000 16%) 2.6182\n",
            "epoch =  5000   loss =  1.4545907392377073\n",
            "2m 59s (- 11m 58s) (5000 20%) 1.4546\n",
            "epoch =  6000   loss =  0.5849378019418099\n",
            "3m 38s (- 11m 31s) (6000 24%) 0.5849\n",
            "epoch =  7000   loss =  0.20838278787206815\n",
            "4m 16s (- 11m 0s) (7000 28%) 0.2084\n",
            "epoch =  8000   loss =  0.08876294900185863\n",
            "4m 55s (- 10m 28s) (8000 32%) 0.0888\n",
            "epoch =  9000   loss =  0.042397074776221226\n",
            "5m 33s (- 9m 52s) (9000 36%) 0.0424\n",
            "epoch =  10000   loss =  0.03163008247664407\n",
            "6m 12s (- 9m 18s) (10000 40%) 0.0316\n",
            "epoch =  11000   loss =  0.026206732651063613\n",
            "6m 50s (- 8m 42s) (11000 44%) 0.0262\n",
            "epoch =  12000   loss =  0.021596345053253693\n",
            "7m 29s (- 8m 6s) (12000 48%) 0.0216\n",
            "epoch =  13000   loss =  0.018867887643336356\n",
            "8m 7s (- 7m 29s) (13000 52%) 0.0189\n",
            "epoch =  14000   loss =  0.016042343215096323\n",
            "8m 45s (- 6m 53s) (14000 56%) 0.0160\n",
            "epoch =  15000   loss =  0.015650530460540046\n",
            "9m 24s (- 6m 16s) (15000 60%) 0.0157\n",
            "epoch =  16000   loss =  0.013359331532879482\n",
            "10m 2s (- 5m 38s) (16000 64%) 0.0134\n",
            "epoch =  17000   loss =  0.011873091339624007\n",
            "10m 40s (- 5m 1s) (17000 68%) 0.0119\n",
            "epoch =  18000   loss =  0.010913109689459702\n",
            "11m 18s (- 4m 23s) (18000 72%) 0.0109\n",
            "epoch =  19000   loss =  0.010103366740719558\n",
            "11m 57s (- 3m 46s) (19000 76%) 0.0101\n",
            "epoch =  20000   loss =  0.009110849553838568\n",
            "12m 34s (- 3m 8s) (20000 80%) 0.0091\n",
            "epoch =  21000   loss =  0.008479037834634343\n",
            "13m 12s (- 2m 30s) (21000 84%) 0.0085\n",
            "epoch =  22000   loss =  0.008041112830916772\n",
            "13m 51s (- 1m 53s) (22000 88%) 0.0080\n",
            "epoch =  23000   loss =  0.007583009874306449\n",
            "14m 28s (- 1m 15s) (23000 92%) 0.0076\n",
            "epoch =  24000   loss =  0.007052608959219093\n",
            "15m 7s (- 0m 37s) (24000 96%) 0.0071\n",
            "epoch =  25000   loss =  0.006790911465345444\n",
            "15m 44s (- 0m 0s) (25000 100%) 0.0068\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_Nn5BTv1xJC",
        "colab_type": "text"
      },
      "source": [
        "Training and Evaluating\n",
        "=======================\n",
        "\n",
        "With all these helper functions in place (it looks like extra work, but\n",
        "it makes it easier to run multiple experiments) we can actually\n",
        "initialize a network and start training.\n",
        "\n",
        "Remember that the input sentences were heavily filtered. For this small\n",
        "dataset we can use relatively small networks of 256 hidden nodes and a\n",
        "single GRU layer. After about 40 minutes on a MacBook CPU we'll get some\n",
        "reasonable results.\n",
        "\n",
        ".. Note::\n",
        "   If you run this notebook you can train, interrupt the kernel,\n",
        "   evaluate, and continue training later. Comment out the lines where the\n",
        "   encoder and decoder are initialized and run ``trainIters`` again.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oi_fCEhU1xJD",
        "colab_type": "code",
        "outputId": "bbf035f8-070d-46c4-db9a-436815d70654",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "no_of_epoch = 25000\n",
        "no_hidden_states = 256\n",
        "model_type = \"AttnDecoder\"\n",
        "model_name_dec =  model_type+\"_Model_\"+str(no_of_epoch)+\"_\"+str(no_hidden_states)\n",
        "torch.save(attn_decoder1.state_dict(), model_name_dec)\n",
        "# device = torch.device('cpu')\n",
        "decoder_model = AttnDecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
        "decoder_model.load_state_dict(torch.load(model_name_dec, map_location=device))"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVJhARfVEeES",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datafile = \"dev_\"\n",
        "# epochs\n",
        "task = \"Project_\"\n",
        "model_name = task+datafile+str(no_of_epoch)+\"_\"+str(no_hidden_states)+\".encoder\"\n",
        "\n",
        "torch.save(encoder1.state_dict(), model_name)\n",
        "device = torch.device('cpu')\n",
        "# encoder_model = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "# encoder_model.load_state_dict(torch.load(model_name, map_location=device))\n",
        "\n",
        "\n",
        "model_name = task+datafile+str(no_of_epoch)+\"_\"+str(no_hidden_states)+\".attndecoder\"\n",
        "torch.save(attn_decoder1.state_dict(), model_name)\n",
        "device = torch.device('cpu')\n",
        "# decoder_model = DecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
        "# decoder_model.load_state_dict(torch.load(model_name, map_location=device))\n",
        "\n",
        "# showPlot(plot_losses, plot_epoch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXaSkCJ01xJJ",
        "colab_type": "code",
        "outputId": "fa753ab0-2d83-452c-f2ca-ca2aa2904b10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 744
        }
      },
      "source": [
        "device = torch.device('cuda')\n",
        "evaluateRandomly(encoder1, attn_decoder1)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> .\n",
            "=  \n",
            "< ये व्यक्ति को एकाग्रचित नहीं होते हैं जो दिमाग को अलग है । <EOS>\n",
            "\n",
            "> .\n",
            "=  \n",
            "< इसका निर्माण उन्नीस , सौ बीस में हुआ था । <EOS>\n",
            "\n",
            "> .\n",
            "=  \n",
            "< इसका निर्माण उन्नीस , सौ बीस में हुआ था । <EOS>\n",
            "\n",
            "> .\n",
            "=  \n",
            "< इसका निर्माण उन्नीस , सौ बीस मिनट तक भी अच्छा होगा । <EOS>\n",
            "\n",
            "> .\n",
            "=  \n",
            "< इसका निर्माण उन्नीस , सौ बीस में हुआ था । <EOS>\n",
            "\n",
            "> .\n",
            "=  \n",
            "< इसका निर्माण उन्नीस , सौ बीस करने के लिए । <EOS>\n",
            "\n",
            "> .\n",
            "=  \n",
            "< इसका निर्माण उन्नीस , सौ बीस का किराया 18 हजार रुपये से भी कम । <EOS>\n",
            "\n",
            "> .\n",
            "=  \n",
            "< इसका निर्माण उन्नीस , सौ इस बार तम्बाकू की जा सकती है । <EOS>\n",
            "\n",
            "> .\n",
            "=  \n",
            "< इसका निर्माण उन्नीस , सौ बीस से बचें । <EOS>\n",
            "\n",
            "> .\n",
            "=  \n",
            "< इसका निर्माण उन्नीस , सौ बीस में हुआ है । <EOS>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYYZqNkuGUJ5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "3c7f50ac-a1eb-4b0d-8f79-7b6887ed11bf"
      },
      "source": [
        "print(epoch_list)\n",
        "print(loss_list)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000, 11000, 12000, 13000, 14000, 15000, 16000, 17000, 18000, 19000, 20000, 21000, 22000, 23000, 24000, 25000]\n",
            "[4.617335979790813, 4.439340661458477, 3.812911378598869, 2.618166730167369, 1.4545907392377073, 0.5849378019418099, 0.20838278787206815, 0.08876294900185863, 0.042397074776221226, 0.03163008247664407, 0.026206732651063613, 0.021596345053253693, 0.018867887643336356, 0.016042343215096323, 0.015650530460540046, 0.013359331532879482, 0.011873091339624007, 0.010913109689459702, 0.010103366740719558, 0.009110849553838568, 0.008479037834634343, 0.008041112830916772, 0.007583009874306449, 0.007052608959219093, 0.006790911465345444]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fi-C89yGW7c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "s = \"epoch_list_\"+model_name_dec+'.pkl' \n",
        "with open(s, 'wb') as f:\n",
        "    pickle.dump(epoch_list, f)\n",
        "\n",
        "s = \"loss_list\"+model_name_dec+'.pkl' \n",
        "with open(s, 'wb') as f:\n",
        "    pickle.dump(loss_list, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEkesrjlGYei",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "outputId": "d737c12d-4538-40f6-c2af-2d380821b6cc"
      },
      "source": [
        "print(\"after loading pickles\")\n",
        "s = \"epoch_list_\"+model_name_dec+'.pkl' \n",
        "with open(s, 'wb') as f:\n",
        "    mynewlist = pickle.load(f)\n",
        "    print(mynewlist)\n",
        "\n",
        "s = \"loss_list\"+model_name_dec+'.pkl' \n",
        "with open(s, 'wb') as f:\n",
        "    mynewlist = pickle.load(f)\n",
        "    print(mynewlist)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "after loading pickles\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "UnsupportedOperation",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnsupportedOperation\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-91-858a39cdce03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"epoch_list_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mmodel_name_dec\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.pkl'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmynewlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmynewlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnsupportedOperation\u001b[0m: read"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovghTwLqGcbO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction    \n",
        "\n",
        "def calculate_bleu(pred_trg, real_trg):\n",
        "    smoothie = SmoothingFunction().method4\n",
        "    score = sentence_bleu(real_trg, pred_trg, smoothing_function=smoothie)\n",
        "    return score \n",
        "\n",
        "def calculate_Result(encoder, decoder,lcl_pairs, n=50):\n",
        "    device = torch.device('cuda')\n",
        "\n",
        "    result_value_bleu_score = []\n",
        "    \n",
        "    for i in range(n):\n",
        "        pair = random.choice(lcl_pairs)\n",
        "        if debug:\n",
        "          print('>', pair[0])\n",
        "          print('=', pair[1])\n",
        "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        if debug:\n",
        "          print('<', output_sentence)\n",
        "        reference = [pair[1].split()]\n",
        "        if debug:\n",
        "          print('--', reference)\n",
        "        output_words = output_words[:-1]\n",
        "        temp  = []\n",
        "        for ow in output_words:\n",
        "          if ow!='':\n",
        "            temp.append(ow)\n",
        "        output_words = temp\n",
        "        target_predicted = output_words\n",
        "        \n",
        "        if debug:        \n",
        "          print('<<', output_words)\n",
        "        \n",
        "        score = calculate_bleu(target_predicted,reference)\n",
        "        \n",
        "        if debug:\n",
        "          print(\"---Value\",score)\n",
        "        \n",
        "        result_value_bleu_score.append((pair[0],pair[1].split(),target_predicted,score))\n",
        "\n",
        "    return result_value_bleu_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThRJgdDOGg0s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def readLangsTest(lang1, lang2, reverse=False):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # Read the file and split into lines  \n",
        "    \n",
        "    # lines1 = open(io.StringIO(uploaded['dev.en'].decode('utf-8')),encoding='utf-8').read().strip().split('\\n')\n",
        "    # lines2 = open(io.StringIO(uploaded['dev.hi'].decode('utf-8')),encoding='utf-8').read().strip().split('\\n')\n",
        "\n",
        "#     lines1 = open('/content/gdrive/My Drive/nlpa/test.en',encoding='utf-8').read().strip().split('\\n')\n",
        "#     lines2 = open('/content/gdrive/My Drive/nlpa/test.hi',encoding='utf-8').read().strip().split('\\n')\n",
        "\n",
        "    test_source = \"/content/gdrive/My Drive/NLPA/NLA S20 - Assignment 2 Data/enghin/dev.en\"\n",
        "    test_target = \"/content/gdrive/My Drive/NLPA/NLA S20 - Assignment 2 Data/enghin/dev.hi\"\n",
        "\n",
        "    lines1 = open(test_source,encoding='utf-8').read().strip().split('\\n')\n",
        "    lines2 = open(test_target,encoding='utf-8').read().strip().split('\\n')\n",
        "\n",
        "    print(\"len lines1 \",len(lines1))\n",
        "    print(\"len lines2 \",len(lines2))\n",
        "    print(\"line2 example\")\n",
        "    print(lines2[0])\n",
        " \n",
        "    lines = []\n",
        "    for i in range(0,len(lines1)):\n",
        "        lines.append(lines1[i]+' \\t '+lines2[i])\n",
        "    print(\"len(lines) \",len(lines))\n",
        "    print(\"lines[0] \",lines[0])\n",
        "    pairs = [[s for s in l.split('\\t')] for l in lines]\n",
        "    print(\"len(pairs) \",len(pairs))\n",
        "    reverse = False\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(lang2)\n",
        "        output_lang = Lang(lang1)\n",
        "    else:\n",
        "        input_lang = Lang(lang1)\n",
        "        output_lang = Lang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80GsCzg5Gkax",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "outputId": "394db3d8-7aa9-4a4d-d81c-f38ab13a2409"
      },
      "source": [
        "def prepareDataTest(lang1, lang2, reverse=False):\n",
        "    input_lang, output_lang, pairs = readLangsTest(lang1, lang2, reverse)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "\n",
        "input_lang, output_lang, pairs = prepareDataTest('eng', 'hi', True)\n",
        "print(random.choice(pairs))"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading lines...\n",
            "len lines1  401\n",
            "len lines2  401\n",
            "line2 example\n",
            "बल्कि आप नेत्ररोगों से भी बचे रहेंगे ।\n",
            "len(lines)  401\n",
            "lines[0]  but you will also be safe from eye diseases . \t बल्कि आप नेत्ररोगों से भी बचे रहेंगे ।\n",
            "len(pairs)  401\n",
            "Read 401 sentence pairs\n",
            "Trimmed to 245 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "eng 1071\n",
            "hi 1117\n",
            "['If two elders are there then children less then 15 years of age absolutely free . ', ' और अगर दो बड़े हों तो 15 साल से कम उम्र के बच्चे बिलकुल फ्री ।']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nh5TRotYGnD2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3f5fe309-8cf7-4edc-93e5-2a7f4ff0adee"
      },
      "source": [
        "global debug\n",
        "debug = 1\n",
        "device = torch.device('cuda')\n",
        "result_value_bleu_score = calculate_Result(encoder1, attn_decoder1,pairs)\n",
        "result_value_bleu_score_dict = {}\n",
        "result_value_bleu_score_dict['result'] = result_value_bleu_score \n",
        "# torch.save(result_value_bleu_score_dict, train_result_data_path)\n",
        "for item in result_value_bleu_score:\n",
        "  if (item[3]>0):\n",
        "    print(\" Source Language \",item[0])\n",
        "    print(\" Input Target\",item[1])\n",
        "    print(\" Output Target\",item[2])\n",
        "    print(\" Score \",item[3])"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> AIDS spreads from these . \n",
            "=  एड्स इनसे फैलता है ।\n",
            "< प्रवेश कतरन जाँच जानते रहेंगे <EOS>\n",
            "-- [['एड्स', 'इनसे', 'फैलता', 'है', '।']]\n",
            "<< ['प्रवेश', 'कतरन', 'जाँच', 'जानते', 'रहेंगे']\n",
            "---Value 0\n",
            "> Themain attractions of Rome are it 's fountains . \n",
            "=  रोम के खास आकर्षण इसके फाउंटेंस हैं ।\n",
            "< प्रवेश मानसिक नजर बादाम एंटीआक्सीडेंट प्रयास यूनिवर्सिटी रखने उसे मूर्तिकार माइकल फायदेमंद ईस्वी दिलचस्प रहेंगे <EOS>\n",
            "-- [['रोम', 'के', 'खास', 'आकर्षण', 'इसके', 'फाउंटेंस', 'हैं', '।']]\n",
            "<< ['प्रवेश', 'मानसिक', 'नजर', 'बादाम', 'एंटीआक्सीडेंट', 'प्रयास', 'यूनिवर्सिटी', 'रखने', 'उसे', 'मूर्तिकार', 'माइकल', 'फायदेमंद', 'ईस्वी', 'दिलचस्प', 'रहेंगे']\n",
            "---Value 0\n",
            "> Prevention of AIDS and propagation . \n",
            "=  एड्स से बचाव एवं प्रचार प्रसार ।\n",
            "< आडियो सुनकर आपके कविता सा सुविधाजनक कहते पहचाने रहेंगे <EOS>\n",
            "-- [['एड्स', 'से', 'बचाव', 'एवं', 'प्रचार', 'प्रसार', '।']]\n",
            "<< ['आडियो', 'सुनकर', 'आपके', 'कविता', 'सा', 'सुविधाजनक', 'कहते', 'पहचाने', 'रहेंगे']\n",
            "---Value 0\n",
            "> Also also learn about the rules of insurance or loss coverage from the tour company . \n",
            "=  साथ ही टूर कंपनी से इंश्योरेंस या नुकसान की भरपाई के नियम भी जान लें ।\n",
            "< अहमदाबाद उसे यूनिवर्सिटी बिस्तर कतरन मॉलरोड अनूठा यूनिवर्सिटी अनुभव दिमाग 601 फायदेमंद ईस्वी दिलचस्प रहेंगे <EOS>\n",
            "-- [['साथ', 'ही', 'टूर', 'कंपनी', 'से', 'इंश्योरेंस', 'या', 'नुकसान', 'की', 'भरपाई', 'के', 'नियम', 'भी', 'जान', 'लें', '।']]\n",
            "<< ['अहमदाबाद', 'उसे', 'यूनिवर्सिटी', 'बिस्तर', 'कतरन', 'मॉलरोड', 'अनूठा', 'यूनिवर्सिटी', 'अनुभव', 'दिमाग', '601', 'फायदेमंद', 'ईस्वी', 'दिलचस्प', 'रहेंगे']\n",
            "---Value 0\n",
            "> we reached at sunset and we made a mind of taking a round of that garden . \n",
            "=  हम शाम ढले पहुँचे थे और हमने एक चक्कर उस बाग के लगाने का इरादा किया ।\n",
            "< प्रारंभ कतरन हाल छोटी निश्‍चित विटामिन-ई कतरन खंडित निदान लगता जानते रहेंगे <EOS>\n",
            "-- [['हम', 'शाम', 'ढले', 'पहुँचे', 'थे', 'और', 'हमने', 'एक', 'चक्कर', 'उस', 'बाग', 'के', 'लगाने', 'का', 'इरादा', 'किया', '।']]\n",
            "<< ['प्रारंभ', 'कतरन', 'हाल', 'छोटी', 'निश्\\u200dचित', 'विटामिन-ई', 'कतरन', 'खंडित', 'निदान', 'लगता', 'जानते', 'रहेंगे']\n",
            "---Value 0\n",
            "> Bilberry or blackberry anthocyanin saves from blindness or cataract . \n",
            "=  बिलबैरी या ब्लैकबैरी एंथोसाइनिन अंधेपन या मोतियाबिंद से बचाती है ।\n",
            "< रगडें यूनिवर्सिटी नहाने फायदेमंद अंदाज  बल्कि सकते कतरन फोन में रहेंगे <EOS>\n",
            "-- [['बिलबैरी', 'या', 'ब्लैकबैरी', 'एंथोसाइनिन', 'अंधेपन', 'या', 'मोतियाबिंद', 'से', 'बचाती', 'है', '।']]\n",
            "<< ['रगडें', 'यूनिवर्सिटी', 'नहाने', 'फायदेमंद', 'अंदाज', 'बल्कि', 'सकते', 'कतरन', 'फोन', 'में', 'रहेंगे']\n",
            "---Value 0\n",
            "> Saw a dry lake in which farmers were farming . \n",
            "=  सूखी झील देखी जिसमें किसान खेती कर रहे थे ।\n",
            "< बढ़ती हैं डील तलाश जानते ब्रोकोली आपसी साथ खा रहेंगे <EOS>\n",
            "-- [['सूखी', 'झील', 'देखी', 'जिसमें', 'किसान', 'खेती', 'कर', 'रहे', 'थे', '।']]\n",
            "<< ['बढ़ती', 'हैं', 'डील', 'तलाश', 'जानते', 'ब्रोकोली', 'आपसी', 'साथ', 'खा', 'रहेंगे']\n",
            "---Value 0\n",
            "> During busy season take special care of this point . \n",
            "=  बिजी सीजन के दौरान तो आप इस बात का खास ख्याल रखें ।\n",
            "< कुजीन यूनिवर्सिटी रखने हैं सर्व छूत इस 55 तत्पश्‍चात नींद से लकवा कंफर्ट रहेंगे <EOS>\n",
            "-- [['बिजी', 'सीजन', 'के', 'दौरान', 'तो', 'आप', 'इस', 'बात', 'का', 'खास', 'ख्याल', 'रखें', '।']]\n",
            "<< ['कुजीन', 'यूनिवर्सिटी', 'रखने', 'हैं', 'सर्व', 'छूत', 'इस', '55', 'तत्पश्\\u200dचात', 'नींद', 'से', 'लकवा', 'कंफर्ट', 'रहेंगे']\n",
            "---Value 0.18968126713037053\n",
            "> Paralysis may be controlled by yoga . \n",
            "=  योग से काबू हो सकता है लकवा ।\n",
            "< चर्च उसे यूनिवर्सिटी तकनीक बढ़ते एच.आई.वी चढ़ाने सीट से लेनी कहते रहेंगे <EOS>\n",
            "-- [['योग', 'से', 'काबू', 'हो', 'सकता', 'है', 'लकवा', '।']]\n",
            "<< ['चर्च', 'उसे', 'यूनिवर्सिटी', 'तकनीक', 'बढ़ते', 'एच.आई.वी', 'चढ़ाने', 'सीट', 'से', 'लेनी', 'कहते', 'रहेंगे']\n",
            "---Value 0.19259074009082888\n",
            "> After the opening of Aman Bagh big personas of our nation and abroad reached here . \n",
            "=  अमन बाग के खुलने के बाद वहाँ देश - विदेश के बड़े-बड़े लोग पहुँचे हैं ।\n",
            "< प्रसिद्ध फ्रूट दवा यूनिवर्सिटी भारतीय गाजर इस :- कतरन महत्त्वपूर्ण रूकता दिलचस्प रहेंगे <EOS>\n",
            "-- [['अमन', 'बाग', 'के', 'खुलने', 'के', 'बाद', 'वहाँ', 'देश', '-', 'विदेश', 'के', 'बड़े-बड़े', 'लोग', 'पहुँचे', 'हैं', '।']]\n",
            "<< ['प्रसिद्ध', 'फ्रूट', 'दवा', 'यूनिवर्सिटी', 'भारतीय', 'गाजर', 'इस', ':-', 'कतरन', 'महत्त्वपूर्ण', 'रूकता', 'दिलचस्प', 'रहेंगे']\n",
            "---Value 0\n",
            "> The symbol of Rome is Colloseum . \n",
            "=  रोम का प्रतीक है कॉलोसियम ।\n",
            "< प्रवेश मानसिक दोस्तों यूनिवर्सिटी भारतीय कुल , पेप्टिक नजर हों मुँह जानते रहेंगे <EOS>\n",
            "-- [['रोम', 'का', 'प्रतीक', 'है', 'कॉलोसियम', '।']]\n",
            "<< ['प्रवेश', 'मानसिक', 'दोस्तों', 'यूनिवर्सिटी', 'भारतीय', 'कुल', ',', 'पेप्टिक', 'नजर', 'हों', 'मुँह', 'जानते', 'रहेंगे']\n",
            "---Value 0\n",
            "> Which method is convenient for you ? \n",
            "=  आपके लिए कौन सा तरीका सुविधाजनक है ।\n",
            "< जरूर होल साइट्स यूनिवर्सिटी भारतीय बल्कि जरूर हर बल्कि काफी नजर पीना जानते रहेंगे <EOS>\n",
            "-- [['आपके', 'लिए', 'कौन', 'सा', 'तरीका', 'सुविधाजनक', 'है', '।']]\n",
            "<< ['जरूर', 'होल', 'साइट्स', 'यूनिवर्सिटी', 'भारतीय', 'बल्कि', 'जरूर', 'हर', 'बल्कि', 'काफी', 'नजर', 'पीना', 'जानते', 'रहेंगे']\n",
            "---Value 0\n",
            "> Which means three day fair for one person is even less than Rs . 18 thousand . \n",
            "=  यानी तीन दिन के लिए एक व्यक्‍ति का किराया 18 हजार रुपये से भी कम ।\n",
            "< शिक्षण बनाये अनिवार्य कहते पहचाने पढ़कर बीमार नजर खुलने इस कहते रहेंगे <EOS>\n",
            "-- [['यानी', 'तीन', 'दिन', 'के', 'लिए', 'एक', 'व्यक्\\u200dति', 'का', 'किराया', '18', 'हजार', 'रुपये', 'से', 'भी', 'कम', '।']]\n",
            "<< ['शिक्षण', 'बनाये', 'अनिवार्य', 'कहते', 'पहचाने', 'पढ़कर', 'बीमार', 'नजर', 'खुलने', 'इस', 'कहते', 'रहेंगे']\n",
            "---Value 0\n",
            "> Spectacles are tested after 6 weeks . \n",
            "=  6 सप्ताह बाद चश्में की जाँच की जाती है ।\n",
            "< गोपनीयता इस आसान फायदेमंद प्रसार कतरन प्रमुख योग्य में रहेंगे <EOS>\n",
            "-- [['6', 'सप्ताह', 'बाद', 'चश्में', 'की', 'जाँच', 'की', 'जाती', 'है', '।']]\n",
            "<< ['गोपनीयता', 'इस', 'आसान', 'फायदेमंद', 'प्रसार', 'कतरन', 'प्रमुख', 'योग्य', 'में', 'रहेंगे']\n",
            "---Value 0\n",
            "> A person becomes infected easily . \n",
            "=  व्यक्ति को बड़ी आसानी से संक्रमण होने लगते हैं ।\n",
            "< निरन्तर बढ़ती यूरोप फायदेमंद अंदाज अंदाजा सब्जियाँ बेमिसाल रहेंगे <EOS>\n",
            "-- [['व्यक्ति', 'को', 'बड़ी', 'आसानी', 'से', 'संक्रमण', 'होने', 'लगते', 'हैं', '।']]\n",
            "<< ['निरन्तर', 'बढ़ती', 'यूरोप', 'फायदेमंद', 'अंदाज', 'अंदाजा', 'सब्जियाँ', 'बेमिसाल', 'रहेंगे']\n",
            "---Value 0\n",
            "> Here almost 300 buildings are such in which magnificent fountains are built . \n",
            "=  यहाँ लगभग 300 इमारतें ऐसी हैं , जिनमें शानदार फव्वारे बने हुए हैं ।\n",
            "< बिना लगने फायदेमंद बढ़ती तरीका प्रयास जानते रहेंगे <EOS>\n",
            "-- [['यहाँ', 'लगभग', '300', 'इमारतें', 'ऐसी', 'हैं', ',', 'जिनमें', 'शानदार', 'फव्वारे', 'बने', 'हुए', 'हैं', '।']]\n",
            "<< ['बिना', 'लगने', 'फायदेमंद', 'बढ़ती', 'तरीका', 'प्रयास', 'जानते', 'रहेंगे']\n",
            "---Value 0\n",
            "> In Vadodra a unique confluence of business and art is seen . \n",
            "=  वडोदरा में व्यापार व कला का अनूठा संगम दिखाई देता है ।\n",
            "< बढ़ती इम्यूनो-डेफिशिएंशी वायरस करतीं क्षमता जा जानकारी फंडे मदद जानते रहेंगे <EOS>\n",
            "-- [['वडोदरा', 'में', 'व्यापार', 'व', 'कला', 'का', 'अनूठा', 'संगम', 'दिखाई', 'देता', 'है', '।']]\n",
            "<< ['बढ़ती', 'इम्यूनो-डेफिशिएंशी', 'वायरस', 'करतीं', 'क्षमता', 'जा', 'जानकारी', 'फंडे', 'मदद', 'जानते', 'रहेंगे']\n",
            "---Value 0\n",
            "> Here not the greenery of tea estates , but the commuting of forests - waterfalls is there . \n",
            "=  यहाँ चाय बागानों की हरियाली नहीं , बल्कि जंगलों - झरनों की आवाजाही है ।\n",
            "< रगडें यूनिवर्सिटी नहाने फायदेमंद फोड़े-फुंसी नसों जिसमें औषधि लसिका कहते रहेंगे <EOS>\n",
            "-- [['यहाँ', 'चाय', 'बागानों', 'की', 'हरियाली', 'नहीं', ',', 'बल्कि', 'जंगलों', '-', 'झरनों', 'की', 'आवाजाही', 'है', '।']]\n",
            "<< ['रगडें', 'यूनिवर्सिटी', 'नहाने', 'फायदेमंद', 'फोड़े-फुंसी', 'नसों', 'जिसमें', 'औषधि', 'लसिका', 'कहते', 'रहेंगे']\n",
            "---Value 0\n",
            "> It is necessary to maintain the secrecy of the persons living with H . . . \n",
            "=  एच.आई.वी. के साथ जी रहे लोगों की गोपनीयता बनाये रखना अनिवार्य है ।\n",
            "< प्रारंभ नेत्ररोगों आवाजाही यूनिवर्सिटी व्यवस्था गार्डन इस शब्दों नजर बनाने फंडे जानते रहेंगे <EOS>\n",
            "-- [['एच.आई.वी.', 'के', 'साथ', 'जी', 'रहे', 'लोगों', 'की', 'गोपनीयता', 'बनाये', 'रखना', 'अनिवार्य', 'है', '।']]\n",
            "<< ['प्रारंभ', 'नेत्ररोगों', 'आवाजाही', 'यूनिवर्सिटी', 'व्यवस्था', 'गार्डन', 'इस', 'शब्दों', 'नजर', 'बनाने', 'फंडे', 'जानते', 'रहेंगे']\n",
            "---Value 0\n",
            "> In comparison with the traditional surgical operation this method has many advantages . \n",
            "=  पारंपरिक शल्यक्रिया के मुकाबले इस विधि के अनेक फायदे हैं ।\n",
            "< यौन अवयवों यूनिवर्सिटी बिस्तर एक नजर ने रखने रहेंगे <EOS>\n",
            "-- [['पारंपरिक', 'शल्यक्रिया', 'के', 'मुकाबले', 'इस', 'विधि', 'के', 'अनेक', 'फायदे', 'हैं', '।']]\n",
            "<< ['यौन', 'अवयवों', 'यूनिवर्सिटी', 'बिस्तर', 'एक', 'नजर', 'ने', 'रखने', 'रहेंगे']\n",
            "---Value 0\n",
            "> In this way you will get much important information from them . \n",
            "=  इस तरह आपको काफी जरूरी जानकारी उनसे मिल जाएगी ।\n",
            "< जरूर पुरानी अनाज इसके बने चार्ट नजर आहार नेत्ररोगों विटामिन-ए मेवे कहते रहेंगे <EOS>\n",
            "-- [['इस', 'तरह', 'आपको', 'काफी', 'जरूरी', 'जानकारी', 'उनसे', 'मिल', 'जाएगी', '।']]\n",
            "<< ['जरूर', 'पुरानी', 'अनाज', 'इसके', 'बने', 'चार्ट', 'नजर', 'आहार', 'नेत्ररोगों', 'विटामिन-ए', 'मेवे', 'कहते', 'रहेंगे']\n",
            "---Value 0\n",
            "> Now I understood that why a tourist come over here after spending so much . \n",
            "=  अब समझ में आया कि टूरिस्ट यहाँ इतना खर्च करके क्यों आता है ।\n",
            "< काबू कतरन करती आँखें बुरक बंद हैं प्रस्तुत लड़कियों फायदेमंद मंत्र उतना जानते रहेंगे <EOS>\n",
            "-- [['अब', 'समझ', 'में', 'आया', 'कि', 'टूरिस्ट', 'यहाँ', 'इतना', 'खर्च', 'करके', 'क्यों', 'आता', 'है', '।']]\n",
            "<< ['काबू', 'कतरन', 'करती', 'आँखें', 'बुरक', 'बंद', 'हैं', 'प्रस्तुत', 'लड़कियों', 'फायदेमंद', 'मंत्र', 'उतना', 'जानते', 'रहेंगे']\n",
            "---Value 0\n",
            "> Join the great travel of the nation against the HIV and AIDS . \n",
            "=  एच.आई.वी. और एड्स के खिलाफ़ देश की महायात्रा में शामिल हों ।\n",
            "< निम्न दुष्परिणाम सामने आते नेत्ररोगों यौगिक इस बार फायदेमंद आघात तभी कहते रहेंगे <EOS>\n",
            "-- [['एच.आई.वी.', 'और', 'एड्स', 'के', 'खिलाफ़', 'देश', 'की', 'महायात्रा', 'में', 'शामिल', 'हों', '।']]\n",
            "<< ['निम्न', 'दुष्परिणाम', 'सामने', 'आते', 'नेत्ररोगों', 'यौगिक', 'इस', 'बार', 'फायदेमंद', 'आघात', 'तभी', 'कहते', 'रहेंगे']\n",
            "---Value 0\n",
            "> With the division in the CD-4 cells many viruses enter into the blood-circulation . \n",
            "=  सीडी-4 कोशिका खंडित हो जाने से अनेक वायरस रक्त-प्रवाह में प्रवेश कर जाते हैं ।\n",
            "< जरूर पुरानी अनाज पथरी प्रस्तुत बखूबी जरूर पैथियॉन फायदेमंद परसेंट स्टाफ अद्‍भुत कहते रहेंगे <EOS>\n",
            "-- [['सीडी-4', 'कोशिका', 'खंडित', 'हो', 'जाने', 'से', 'अनेक', 'वायरस', 'रक्त-प्रवाह', 'में', 'प्रवेश', 'कर', 'जाते', 'हैं', '।']]\n",
            "<< ['जरूर', 'पुरानी', 'अनाज', 'पथरी', 'प्रस्तुत', 'बखूबी', 'जरूर', 'पैथियॉन', 'फायदेमंद', 'परसेंट', 'स्टाफ', 'अद्\\u200dभुत', 'कहते', 'रहेंगे']\n",
            "---Value 0\n",
            "> In the middle of this huge dome there is a wide place of 30 feet . \n",
            "=  इसके विशाल गुंबज के मध्य में 30 फुट का खुला स्थान है ।\n",
            "< प्रवेश यूनिवर्सिटी रखने नेत्रशल्यक एक नजर आँखें बुरक नेत्ररोगों से पथरी असुरक्षित जानते रहेंगे <EOS>\n",
            "-- [['इसके', 'विशाल', 'गुंबज', 'के', 'मध्य', 'में', '30', 'फुट', 'का', 'खुला', 'स्थान', 'है', '।']]\n",
            "<< ['प्रवेश', 'यूनिवर्सिटी', 'रखने', 'नेत्रशल्यक', 'एक', 'नजर', 'आँखें', 'बुरक', 'नेत्ररोगों', 'से', 'पथरी', 'असुरक्षित', 'जानते', 'रहेंगे']\n",
            "---Value 0\n",
            "> Carry on bags are good , take them . \n",
            "=  कैरी ऑन बैग्स सही रहते हैं , लेकर चलें ।\n",
            "< भारतीय देनी इसलिए देखा लक्षण नेत्ररोगों हों का जानते रहेंगे <EOS>\n",
            "-- [['कैरी', 'ऑन', 'बैग्स', 'सही', 'रहते', 'हैं', ',', 'लेकर', 'चलें', '।']]\n",
            "<< ['भारतीय', 'देनी', 'इसलिए', 'देखा', 'लक्षण', 'नेत्ररोगों', 'हों', 'का', 'जानते', 'रहेंगे']\n",
            "---Value 0\n",
            "> Details of some important symptoms are given below . \n",
            "=  कुछ महत्वपूर्ण लक्षणों का विवरण नीचे दिया जा रहा है ।\n",
            "< प्रारंभ नेत्ररोगों सैर का एक्सरसाइज जा सामुदायिक सेवा भोजन रहेंगे <EOS>\n",
            "-- [['कुछ', 'महत्वपूर्ण', 'लक्षणों', 'का', 'विवरण', 'नीचे', 'दिया', 'जा', 'रहा', 'है', '।']]\n",
            "<< ['प्रारंभ', 'नेत्ररोगों', 'सैर', 'का', 'एक्सरसाइज', 'जा', 'सामुदायिक', 'सेवा', 'भोजन', 'रहेंगे']\n",
            "---Value 0.2103957290011624\n",
            "> Manager fixed our program for the morning . \n",
            "=  मैनेजर ने सुबह के लिए हमारा कार्यक्रम तय कर दिया ।\n",
            "< उच्चारण ढले कतरन करती आँखें बुरक इस :- दिलचस्प रहेंगे <EOS>\n",
            "-- [['मैनेजर', 'ने', 'सुबह', 'के', 'लिए', 'हमारा', 'कार्यक्रम', 'तय', 'कर', 'दिया', '।']]\n",
            "<< ['उच्चारण', 'ढले', 'कतरन', 'करती', 'आँखें', 'बुरक', 'इस', ':-', 'दिलचस्प', 'रहेंगे']\n",
            "---Value 0\n",
            "> Tou trains enhance the journey in the tour of Darjeeling . \n",
            "=  दार्जिलिंग की सैर में ट्‍वाय ट्रेनें सफर में चार चाँद लगा देती है ।\n",
            "< बढ़ती बीच सेवन मन जा बढ़ती बीमार नजर को स्वस्थ बनाए यूनिवर्सिटी रखने रहेंगे <EOS>\n",
            "-- [['दार्जिलिंग', 'की', 'सैर', 'में', 'ट्\\u200dवाय', 'ट्रेनें', 'सफर', 'में', 'चार', 'चाँद', 'लगा', 'देती', 'है', '।']]\n",
            "<< ['बढ़ती', 'बीच', 'सेवन', 'मन', 'जा', 'बढ़ती', 'बीमार', 'नजर', 'को', 'स्वस्थ', 'बनाए', 'यूनिवर्सिटी', 'रखने', 'रहेंगे']\n",
            "---Value 0\n",
            "> AIDS spreads through unprotected sex relations . \n",
            "=  असुरक्षित यौन संबंध से एड्स फैलता है ।\n",
            "< प्रारंभ नेत्ररोगों सैर प्रस्तुत हमें इस आसानी कहते रहेंगे <EOS>\n",
            "-- [['असुरक्षित', 'यौन', 'संबंध', 'से', 'एड्स', 'फैलता', 'है', '।']]\n",
            "<< ['प्रारंभ', 'नेत्ररोगों', 'सैर', 'प्रस्तुत', 'हमें', 'इस', 'आसानी', 'कहते', 'रहेंगे']\n",
            "---Value 0\n",
            "> Fever does not go down , diarrhoea does not stop , weight decreases . \n",
            "=  बुखार नहीं जाता , डायरिया नहीं रूकता , वजन कम हो जाता है ।\n",
            "< प्रतिशत ब्रोकोली गंभीर पहुँचाता प्रस्तुत खाँसी एकाएक नेत्ररोगों अच्छी तरीका बनाए फायदेमंद सी ब्लैकबैरी कैसा एंथोसाइनिन रहेंगे <EOS>\n",
            "-- [['बुखार', 'नहीं', 'जाता', ',', 'डायरिया', 'नहीं', 'रूकता', ',', 'वजन', 'कम', 'हो', 'जाता', 'है', '।']]\n",
            "<< ['प्रतिशत', 'ब्रोकोली', 'गंभीर', 'पहुँचाता', 'प्रस्तुत', 'खाँसी', 'एकाएक', 'नेत्ररोगों', 'अच्छी', 'तरीका', 'बनाए', 'फायदेमंद', 'सी', 'ब्लैकबैरी', 'कैसा', 'एंथोसाइनिन', 'रहेंगे']\n",
            "---Value 0\n",
            "> After this we reached Bhaangadh by Jeep . \n",
            "=  उसके बाद जीप से हम भानगढ़ पहुँचे ।\n",
            "< इस्तेमाल हर अंधता जानते है किसान देखें रहेंगे <EOS>\n",
            "-- [['उसके', 'बाद', 'जीप', 'से', 'हम', 'भानगढ़', 'पहुँचे', '।']]\n",
            "<< ['इस्तेमाल', 'हर', 'अंधता', 'जानते', 'है', 'किसान', 'देखें', 'रहेंगे']\n",
            "---Value 0\n",
            "> H . . . AIDS is not a contagious disease . \n",
            "=  एच.आई.वी. एड्स छूत का रोग नहीं है ।\n",
            "< प्रवेश रह फायदेमंद फव्वारे प्रसिद्ध फव्वारा लिहाज फंडे कहते रहेंगे <EOS>\n",
            "-- [['एच.आई.वी.', 'एड्स', 'छूत', 'का', 'रोग', 'नहीं', 'है', '।']]\n",
            "<< ['प्रवेश', 'रह', 'फायदेमंद', 'फव्वारे', 'प्रसिद्ध', 'फव्वारा', 'लिहाज', 'फंडे', 'कहते', 'रहेंगे']\n",
            "---Value 0\n",
            "> There are many such diseases which have no treatment . \n",
            "=  ऐसी कई बीमारियाँ हैं जिनका इलाज नहीं है ।\n",
            "< प्रवेश इस केन्द्रों उपलब्ध इस केन्द्रों उपलब्ध नजर अंधता जानते रहेंगे <EOS>\n",
            "-- [['ऐसी', 'कई', 'बीमारियाँ', 'हैं', 'जिनका', 'इलाज', 'नहीं', 'है', '।']]\n",
            "<< ['प्रवेश', 'इस', 'केन्द्रों', 'उपलब्ध', 'इस', 'केन्द्रों', 'उपलब्ध', 'नजर', 'अंधता', 'जानते', 'रहेंगे']\n",
            "---Value 0\n",
            "> Inside the temple on all sides idols of Gods and Godesses are made . \n",
            "=  मंदिर के अंदर चारों ओर देवी - देवताओं की मूर्तियाँ बनी हुई हैं ।\n",
            "< शिक्षण केंद्र इस्तेमाल तमाम चर्च इमारतें लैस वडोदरा करें उतना जानते रहेंगे <EOS>\n",
            "-- [['मंदिर', 'के', 'अंदर', 'चारों', 'ओर', 'देवी', '-', 'देवताओं', 'की', 'मूर्तियाँ', 'बनी', 'हुई', 'हैं', '।']]\n",
            "<< ['शिक्षण', 'केंद्र', 'इस्तेमाल', 'तमाम', 'चर्च', 'इमारतें', 'लैस', 'वडोदरा', 'करें', 'उतना', 'जानते', 'रहेंगे']\n",
            "---Value 0\n",
            "> Themain attractions of Rome are it 's fountains . \n",
            "=  रोम के खास आकर्षण इसके फाउंटेंस हैं ।\n",
            "< प्रवेश मानसिक केन्द्रों फायदेमंद बढ़ती यौगिक इस भूमि नजर उसे फायदेमंद परसेंट स्टाफ इलाके अचानक जानते रहेंगे <EOS>\n",
            "-- [['रोम', 'के', 'खास', 'आकर्षण', 'इसके', 'फाउंटेंस', 'हैं', '।']]\n",
            "<< ['प्रवेश', 'मानसिक', 'केन्द्रों', 'फायदेमंद', 'बढ़ती', 'यौगिक', 'इस', 'भूमि', 'नजर', 'उसे', 'फायदेमंद', 'परसेंट', 'स्टाफ', 'इलाके', 'अचानक', 'जानते', 'रहेंगे']\n",
            "---Value 0\n",
            "> Such a glimpse of village life was a wonderful experience for Jackie . \n",
            "=  जेकी के लिए तो ग्रामीण जीवन की ऐसी झलक अनूठा अनुभव था ।\n",
            "< चर्च इस वजह नेत्ररोगों अंदाज किराये नाश्ता कहते पहचाने तरह बोल कहते रहेंगे <EOS>\n",
            "-- [['जेकी', 'के', 'लिए', 'तो', 'ग्रामीण', 'जीवन', 'की', 'ऐसी', 'झलक', 'अनूठा', 'अनुभव', 'था', '।']]\n",
            "<< ['चर्च', 'इस', 'वजह', 'नेत्ररोगों', 'अंदाज', 'किराये', 'नाश्ता', 'कहते', 'पहचाने', 'तरह', 'बोल', 'कहते', 'रहेंगे']\n",
            "---Value 0\n",
            "> Try to remember any information through words and pictures . \n",
            "=  किसी भी जानकारी को शब्दों और चित्र के माध्यम से याद रखने का प्रयास करें ।\n",
            "< संस्कृति कहा है किसान देखें बनीं बड़ी-बड़ी नजर हैं फायदेमंद आडियो स्थान यहाँ एंथोसाइनिन जानते रहेंगे <EOS>\n",
            "-- [['किसी', 'भी', 'जानकारी', 'को', 'शब्दों', 'और', 'चित्र', 'के', 'माध्यम', 'से', 'याद', 'रखने', 'का', 'प्रयास', 'करें', '।']]\n",
            "<< ['संस्कृति', 'कहा', 'है', 'किसान', 'देखें', 'बनीं', 'बड़ी-बड़ी', 'नजर', 'हैं', 'फायदेमंद', 'आडियो', 'स्थान', 'यहाँ', 'एंथोसाइनिन', 'जानते', 'रहेंगे']\n",
            "---Value 0\n",
            "> Now I understood that why a tourist come over here after spending so much . \n",
            "=  अब समझ में आया कि टूरिस्ट यहाँ इतना खर्च करके क्यों आता है ।\n",
            "< काबू कतरन करती आँखें बुरक बंद हैं भर इस मशहूर जा प्रथम विश्‍व का जानते रहेंगे <EOS>\n",
            "-- [['अब', 'समझ', 'में', 'आया', 'कि', 'टूरिस्ट', 'यहाँ', 'इतना', 'खर्च', 'करके', 'क्यों', 'आता', 'है', '।']]\n",
            "<< ['काबू', 'कतरन', 'करती', 'आँखें', 'बुरक', 'बंद', 'हैं', 'भर', 'इस', 'मशहूर', 'जा', 'प्रथम', 'विश्\\u200dव', 'का', 'जानते', 'रहेंगे']\n",
            "---Value 0\n",
            "> If you keep an interest in artefacts then do look at it . \n",
            "=  अगर आप कलाकृतियों में दिलचस्पी रखते हैं , तो उसे जरूर देखें ।\n",
            "< आई अंधेपन इसलिए सिंड्रोम बचाता यूनिवर्सिटी रखने इन्ही चीजों जानते रहेंगे <EOS>\n",
            "-- [['अगर', 'आप', 'कलाकृतियों', 'में', 'दिलचस्पी', 'रखते', 'हैं', ',', 'तो', 'उसे', 'जरूर', 'देखें', '।']]\n",
            "<< ['आई', 'अंधेपन', 'इसलिए', 'सिंड्रोम', 'बचाता', 'यूनिवर्सिटी', 'रखने', 'इन्ही', 'चीजों', 'जानते', 'रहेंगे']\n",
            "---Value 0\n",
            "> Prevention of AIDS and propagation . \n",
            "=  एड्स से बचाव एवं प्रचार प्रसार ।\n",
            "< बढ़ती यौगिक फायदेमंद फोड़े-फुंसी नसों कम आकार कहते रहेंगे <EOS>\n",
            "-- [['एड्स', 'से', 'बचाव', 'एवं', 'प्रचार', 'प्रसार', '।']]\n",
            "<< ['बढ़ती', 'यौगिक', 'फायदेमंद', 'फोड़े-फुंसी', 'नसों', 'कम', 'आकार', 'कहते', 'रहेंगे']\n",
            "---Value 0\n",
            "> Themost beautiful is its lighting . \n",
            "=  सबसे सुंदर है उनकी लाइटिंग ।\n",
            "< रखकर कर जानते है कि रहेंगे <EOS>\n",
            "-- [['सबसे', 'सुंदर', 'है', 'उनकी', 'लाइटिंग', '।']]\n",
            "<< ['रखकर', 'कर', 'जानते', 'है', 'कि', 'रहेंगे']\n",
            "---Value 0.19953087735062713\n",
            "> Paralysis may be controlled by yoga . \n",
            "=  योग से काबू हो सकता है लकवा ।\n",
            "< चर्च उसे यूनिवर्सिटी बसा हैं बल्कि जरूर हर कतरन हमने बतासिया रहेंगे <EOS>\n",
            "-- [['योग', 'से', 'काबू', 'हो', 'सकता', 'है', 'लकवा', '।']]\n",
            "<< ['चर्च', 'उसे', 'यूनिवर्सिटी', 'बसा', 'हैं', 'बल्कि', 'जरूर', 'हर', 'कतरन', 'हमने', 'बतासिया', 'रहेंगे']\n",
            "---Value 0\n",
            "> Discrimination with the infected person is unjust and the indicator of our unawareness . \n",
            "=  संक्रमित व्यक्ति के साथ भेदभाव नाइंसाफी है और हमारी अनभिज्ञता का संकेत ।\n",
            "< बढ़ती लगता जानते है कि प्रयास यूनिवर्सिटी रखने सुगंध कतरन दौरान जानते रहेंगे <EOS>\n",
            "-- [['संक्रमित', 'व्यक्ति', 'के', 'साथ', 'भेदभाव', 'नाइंसाफी', 'है', 'और', 'हमारी', 'अनभिज्ञता', 'का', 'संकेत', '।']]\n",
            "<< ['बढ़ती', 'लगता', 'जानते', 'है', 'कि', 'प्रयास', 'यूनिवर्सिटी', 'रखने', 'सुगंध', 'कतरन', 'दौरान', 'जानते', 'रहेंगे']\n",
            "---Value 0.19112328397109352\n",
            "> What happens when a person gets infected with the H . . . \n",
            "=  क्या होता है , जब किसी व्यक्ति को एच.आई.वी. संक्रमण हो जाता है ?\n",
            "< जरूर अप्रभावित मानसिक जरूर अप्रभावित आमतौर पैप जा दोनों प्रसन्न रहेंगे <EOS>\n",
            "-- [['क्या', 'होता', 'है', ',', 'जब', 'किसी', 'व्यक्ति', 'को', 'एच.आई.वी.', 'संक्रमण', 'हो', 'जाता', 'है', '?']]\n",
            "<< ['जरूर', 'अप्रभावित', 'मानसिक', 'जरूर', 'अप्रभावित', 'आमतौर', 'पैप', 'जा', 'दोनों', 'प्रसन्न', 'रहेंगे']\n",
            "---Value 0\n",
            "> Aman Baag is in real a getaway with peace calm and all comforts and facilities . \n",
            "=  अमन बाग सच में अमन चैन और सारी सुख सुविधाओं वालों वाली सैरगाह है ।\n",
            "< आडियो लेने जानते ब्रोकोली इंश्योरेंस भरपाई नियम यूनिवर्सिटी रखने रहेंगे <EOS>\n",
            "-- [['अमन', 'बाग', 'सच', 'में', 'अमन', 'चैन', 'और', 'सारी', 'सुख', 'सुविधाओं', 'वालों', 'वाली', 'सैरगाह', 'है', '।']]\n",
            "<< ['आडियो', 'लेने', 'जानते', 'ब्रोकोली', 'इंश्योरेंस', 'भरपाई', 'नियम', 'यूनिवर्सिटी', 'रखने', 'रहेंगे']\n",
            "---Value 0\n",
            "> This is a million dollar priceless moment . \n",
            "=  ये तो मिलियन डॉलर अनमोल पल हैं ।\n",
            "< प्रारंभ नेत्ररोगों आवाजाही यूनिवर्सिटी चोट हैं नजर एकाएक कहते रहेंगे <EOS>\n",
            "-- [['ये', 'तो', 'मिलियन', 'डॉलर', 'अनमोल', 'पल', 'हैं', '।']]\n",
            "<< ['प्रारंभ', 'नेत्ररोगों', 'आवाजाही', 'यूनिवर्सिटी', 'चोट', 'हैं', 'नजर', 'एकाएक', 'कहते', 'रहेंगे']\n",
            "---Value 0.19552795980276136\n",
            "> In the button hole surgery the patient faces much less pain after operation . \n",
            "=  बटन होल सर्जरी में मरीज को ऑपरेशन के पश्‍चात का दर्द बहुत कम होता है ।\n",
            "< आदि एवं स्लिप डिस्क का एक्सरसाइज जा दोनों प्रसन्न रहेंगे <EOS>\n",
            "-- [['बटन', 'होल', 'सर्जरी', 'में', 'मरीज', 'को', 'ऑपरेशन', 'के', 'पश्\\u200dचात', 'का', 'दर्द', 'बहुत', 'कम', 'होता', 'है', '।']]\n",
            "<< ['आदि', 'एवं', 'स्लिप', 'डिस्क', 'का', 'एक्सरसाइज', 'जा', 'दोनों', 'प्रसन्न', 'रहेंगे']\n",
            "---Value 0.10730801952148049\n",
            "> Brain exercise works on this very fundamental . \n",
            "=  ब्रेन एक्सरसाइज इसी फंडे पर काम करती है ।\n",
            "< आदि का चिकित्सा गाजर नेत्ररोगों समझ का वर्ष रहेंगे <EOS>\n",
            "-- [['ब्रेन', 'एक्सरसाइज', 'इसी', 'फंडे', 'पर', 'काम', 'करती', 'है', '।']]\n",
            "<< ['आदि', 'का', 'चिकित्सा', 'गाजर', 'नेत्ररोगों', 'समझ', 'का', 'वर्ष', 'रहेंगे']\n",
            "---Value 0\n",
            "> Brain can also be kept sharpened by adopting some good habits . \n",
            "=  कुछ अच्छी आदतें डालकर भी दिमाग को तेज रखा जा सकता है ।\n",
            "< हों यूनिवर्सिटी रखने नेत्रशल्यक एक धीरे इस :- नजर को स्वस्थ बनाए फायदेमंद परसेंट में रहेंगे <EOS>\n",
            "-- [['कुछ', 'अच्छी', 'आदतें', 'डालकर', 'भी', 'दिमाग', 'को', 'तेज', 'रखा', 'जा', 'सकता', 'है', '।']]\n",
            "<< ['हों', 'यूनिवर्सिटी', 'रखने', 'नेत्रशल्यक', 'एक', 'धीरे', 'इस', ':-', 'नजर', 'को', 'स्वस्थ', 'बनाए', 'फायदेमंद', 'परसेंट', 'में', 'रहेंगे']\n",
            "---Value 0.18690518620528904\n",
            " Source Language  During busy season take special care of this point . \n",
            " Input Target ['बिजी', 'सीजन', 'के', 'दौरान', 'तो', 'आप', 'इस', 'बात', 'का', 'खास', 'ख्याल', 'रखें', '।']\n",
            " Output Target ['कुजीन', 'यूनिवर्सिटी', 'रखने', 'हैं', 'सर्व', 'छूत', 'इस', '55', 'तत्पश्\\u200dचात', 'नींद', 'से', 'लकवा', 'कंफर्ट', 'रहेंगे']\n",
            " Score  0.18968126713037053\n",
            " Source Language  Paralysis may be controlled by yoga . \n",
            " Input Target ['योग', 'से', 'काबू', 'हो', 'सकता', 'है', 'लकवा', '।']\n",
            " Output Target ['चर्च', 'उसे', 'यूनिवर्सिटी', 'तकनीक', 'बढ़ते', 'एच.आई.वी', 'चढ़ाने', 'सीट', 'से', 'लेनी', 'कहते', 'रहेंगे']\n",
            " Score  0.19259074009082888\n",
            " Source Language  Details of some important symptoms are given below . \n",
            " Input Target ['कुछ', 'महत्वपूर्ण', 'लक्षणों', 'का', 'विवरण', 'नीचे', 'दिया', 'जा', 'रहा', 'है', '।']\n",
            " Output Target ['प्रारंभ', 'नेत्ररोगों', 'सैर', 'का', 'एक्सरसाइज', 'जा', 'सामुदायिक', 'सेवा', 'भोजन', 'रहेंगे']\n",
            " Score  0.2103957290011624\n",
            " Source Language  Themost beautiful is its lighting . \n",
            " Input Target ['सबसे', 'सुंदर', 'है', 'उनकी', 'लाइटिंग', '।']\n",
            " Output Target ['रखकर', 'कर', 'जानते', 'है', 'कि', 'रहेंगे']\n",
            " Score  0.19953087735062713\n",
            " Source Language  Discrimination with the infected person is unjust and the indicator of our unawareness . \n",
            " Input Target ['संक्रमित', 'व्यक्ति', 'के', 'साथ', 'भेदभाव', 'नाइंसाफी', 'है', 'और', 'हमारी', 'अनभिज्ञता', 'का', 'संकेत', '।']\n",
            " Output Target ['बढ़ती', 'लगता', 'जानते', 'है', 'कि', 'प्रयास', 'यूनिवर्सिटी', 'रखने', 'सुगंध', 'कतरन', 'दौरान', 'जानते', 'रहेंगे']\n",
            " Score  0.19112328397109352\n",
            " Source Language  This is a million dollar priceless moment . \n",
            " Input Target ['ये', 'तो', 'मिलियन', 'डॉलर', 'अनमोल', 'पल', 'हैं', '।']\n",
            " Output Target ['प्रारंभ', 'नेत्ररोगों', 'आवाजाही', 'यूनिवर्सिटी', 'चोट', 'हैं', 'नजर', 'एकाएक', 'कहते', 'रहेंगे']\n",
            " Score  0.19552795980276136\n",
            " Source Language  In the button hole surgery the patient faces much less pain after operation . \n",
            " Input Target ['बटन', 'होल', 'सर्जरी', 'में', 'मरीज', 'को', 'ऑपरेशन', 'के', 'पश्\\u200dचात', 'का', 'दर्द', 'बहुत', 'कम', 'होता', 'है', '।']\n",
            " Output Target ['आदि', 'एवं', 'स्लिप', 'डिस्क', 'का', 'एक्सरसाइज', 'जा', 'दोनों', 'प्रसन्न', 'रहेंगे']\n",
            " Score  0.10730801952148049\n",
            " Source Language  Brain can also be kept sharpened by adopting some good habits . \n",
            " Input Target ['कुछ', 'अच्छी', 'आदतें', 'डालकर', 'भी', 'दिमाग', 'को', 'तेज', 'रखा', 'जा', 'सकता', 'है', '।']\n",
            " Output Target ['हों', 'यूनिवर्सिटी', 'रखने', 'नेत्रशल्यक', 'एक', 'धीरे', 'इस', ':-', 'नजर', 'को', 'स्वस्थ', 'बनाए', 'फायदेमंद', 'परसेंट', 'में', 'रहेंगे']\n",
            " Score  0.18690518620528904\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssNP3bUiGnJS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tVqh7we1xJT",
        "colab_type": "text"
      },
      "source": [
        "For a better viewing experience we will do the extra work of adding axes\n",
        "and labels:\n",
        "\n",
        "\n"
      ]
    }
  ]
}