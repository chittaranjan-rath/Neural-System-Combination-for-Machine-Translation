{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "BiDir_NMT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sY5mSUvX1xGc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "debug = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXUDm2s85ZNZ",
        "colab_type": "code",
        "outputId": "f7a8ee0f-c54c-4546-d59d-4ea1f2c98fc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CXKVJL21xGu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwEmFj511xHA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "UNK_token = 2\n",
        "PAD_token = 3\n",
        "# MAX_LENGTH = 1000\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\", 2:\"UNK\"}\n",
        "        self.n_words = 3  # Count SOS and EOS and UNK\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQMRS-ktqCB6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s\n",
        "\n",
        "def Reverse(lst): \n",
        "    return [ele for ele in reversed(lst)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78wxulOl1xHY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def readLangs(lang1, lang2, reverse=False, model=\"dev\"):\n",
        "    \n",
        "    # if model == \"dev\":\n",
        "    #     source = \"/content/gdrive/My Drive/NLPA/EndSemProject/pruned_DataSet/dev_pruned.en\"\n",
        "    #     target = \"/content/gdrive/My Drive/NLPA/EndSemProject/pruned_DataSet/dev_pruned.hi\"\n",
        "    # else \n",
        "    if model == \"test\":\n",
        "        source = \"/content/gdrive/My Drive/NLPA/EndSemProject/pruned_DataSet/test_pruned.en\"\n",
        "        target = \"/content/gdrive/My Drive/NLPA/EndSemProject/pruned_DataSet/test_pruned.hi\"\n",
        "    else:\n",
        "        source = \"/content/gdrive/My Drive/NLPA/EndSemProject/pruned_DataSet/train_pruned.en\"\n",
        "        target = \"/content/gdrive/My Drive/NLPA/EndSemProject/pruned_DataSet/train_pruned.hi\"\n",
        "    \n",
        "    eng_lines = open(source, encoding='utf-8').read().strip().split('\\n')\n",
        "    hin_lines = open(target, encoding='utf-8').read().strip().split('\\n')\n",
        "\n",
        "    lines = []\n",
        "    for i in range(len(eng_lines)):\n",
        "        lines.append(eng_lines[i] + '\\t' + hin_lines[i])\n",
        "\n",
        "    # Split every line into pairs and normalize\n",
        "    # pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "    pairs = [[s for s in l.split('\\t')] for l in lines]\n",
        "    #print(pairs)\n",
        "    # Reverse pairs, make Lang instances\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(lang2)\n",
        "        output_lang = Lang(lang1)\n",
        "    else:\n",
        "        input_lang = Lang(lang1)\n",
        "        output_lang = Lang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37LCeZbCCi9q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LENGTH = 50\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH \n",
        "\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPT1Pzc11xHq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepareData(lang1, lang2, reverse=False, model=\"dev\", filter_sentence=False):\n",
        "    \n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse, model)\n",
        "    #print(input_lang, output_lang, pairs)\n",
        "    if(filter_sentence):\n",
        "        pairs = filterPairs(pairs)\n",
        "    \n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    \n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    \n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    \n",
        "    return input_lang, output_lang, pairs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcRQAZf9DB4O",
        "colab_type": "code",
        "outputId": "76a6aa80-5c19-45da-b2cc-29db26a8e4cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        }
      },
      "source": [
        "input_lang, output_lang, pairs = prepareData('eng', 'hi', reverse=False, model=\"train\", filter_sentence=True)\n",
        "print(random.choice(pairs))\n",
        "orig_pairs = []\n",
        "for p in pairs:\n",
        "    temp = str(p[0]).split(' ')\n",
        "    temp = Reverse(temp)\n",
        "    listToStr = ' '.join([str(elem) for elem in temp]) \n",
        "#     print(\"listToStr\")\n",
        "#     print(listToStr)\n",
        "    listToStr = str(listToStr)\n",
        "    orig_pairs.append(listToStr+'\\t'+str(p[1]))\n",
        "#     print(orig_pairs[-1])\n",
        "print(\"orig_pairs\")\n",
        "print(orig_pairs[0])\n",
        "print(random.choice(orig_pairs))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Read 437307 sentence pairs\n",
            "Trimmed to 437307 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "eng 127408\n",
            "hi 162700\n",
            "['A craniometric point on the sagittal suture between the parietal foramina . ', 'सममितार्धी सीवन पर पार्श्विक रन्ध्रक के मध्य कपालीय बिन्दु']\n",
            "orig_pairs\n",
            " . friends and children , wives like relations close and objects mundane towards directed being capacity this find generally We\tप्रायः हम इस भंडार को लौकिक विषयों पर और पत्नी , पुत्र मित्र आदि सगे संबंधियों पर प्रवर्तित कर देते हैं । \n",
            " . Varieties Yielding High of Propagation\tउच्च उत्पादन किस्मों का विकास\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpSD5Bb_1xHz",
        "colab_type": "text"
      },
      "source": [
        "The Encoder\n",
        "-----------\n",
        "\n",
        "The encoder of a seq2seq network is a RNN that outputs some value for\n",
        "every word from the input sentence. For every input word the encoder\n",
        "outputs a vector and a hidden state, and uses the hidden state for the\n",
        "next input word.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDZgT8g_1xH1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers=1, dropout=0.1):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, num_layers, bidirectional=True, dropout=dropout) # batch_first=False,\n",
        "        # self.gru = nn.GRU(self.hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedding = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedding\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "        # packed = pack_padded_sequence(x, lengths, batch_first=True)\n",
        "        # output, final = self.gru(packed)\n",
        "        # output, _ = pad_packed_sequence(output, batch_first=True)\n",
        "\n",
        "        # Manually concatenating the final states for both directions\n",
        "        # fwd_final = final[0:final.size(0):2]\n",
        "        # bwd_final = final[1:final.size(0):2]\n",
        "        # final = torch.cat([fwd_final, bwd_final], dim=2)  # [num_layers, batch, 2*dim]\n",
        "\n",
        "        # return output, final\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(self.num_layers*2, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XajZxxXr1xIF",
        "colab_type": "text"
      },
      "source": [
        "The Attention Decoder\n",
        "---------------------\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2HAV8oS1xIH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, num_layers=1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.num_layers = num_layers\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size*2)\n",
        "\n",
        "        # self.pre_output_layer = nn.Linear(hidden_size + 2*hidden_size + emb_size, hidden_size, bias=False)\n",
        "\n",
        "        # Attention -- general/bilinear\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 4, self.hidden_size)\n",
        "        self.attn_general = nn.Linear(self.max_length, self.max_length)\n",
        "        self.lcl_wa_into_hs = nn.Linear(self.hidden_size*2, self.hidden_size*2)\n",
        "\n",
        "        # self.gru = nn.GRU(emb_size + 2*hidden_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size, self.num_layers, bidirectional=True)\n",
        "        self.out = nn.Linear(self.hidden_size*2, self.output_size)\n",
        "\n",
        "        # self.attn_coverage = nn.Linear(self.max_length, self.hidden_size)\n",
        "        # self.attn_coverage_cat = nn.Linear(self.hidden_size*3, self.max_length)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "       \n",
        "        alphas = self.attn_general(torch.matmul(self.lcl_wa_into_hs(embedded[0]), encoder_outputs.T))\n",
        "        \n",
        "        attn_weights = F.softmax(alphas, dim = 1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(self.num_layers*2, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHlWxIcA1xIM",
        "colab_type": "text"
      },
      "source": [
        "Training\n",
        "========\n",
        "\n",
        "Preparing Training Data\n",
        "-----------------------\n",
        "To train, for each pair we will need an input tensor (indexes of the\n",
        "words in the input sentence) and target tensor (indexes of the words in\n",
        "the target sentence). While creating these vectors we will append the\n",
        "EOS token to both sequences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUxlLxyS1xIR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] if word in lang.word2index else UNK_token for word in sentence.split(' ')]\n",
        "\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMNj3Uiq1xIX",
        "colab_type": "text"
      },
      "source": [
        "Training the Model\n",
        "------------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLz4jGbX1xIY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size*2, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "    # decoder_hidden = encoder_outputs\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            # For Paper 2 \n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            \n",
        "            \n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            # For Paper 2 \n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            \n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzusY2u01xId",
        "colab_type": "text"
      },
      "source": [
        "This is a helper function to print time elapsed and estimated time\n",
        "remaining given the current time and progress %.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNovrvZq1xIf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7_F87qsFeOc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_list = []\n",
        "epoch_list = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ib6gNJsH1xIk",
        "colab_type": "text"
      },
      "source": [
        "The whole training process looks like this:\n",
        "\n",
        "-  Start a timer\n",
        "-  Initialize optimizers and criterion\n",
        "-  Create set of training pairs\n",
        "-  Start empty losses array for plotting\n",
        "\n",
        "Then we call ``train`` many times and occasionally print the progress (%\n",
        "of examples, time so far, estimated time) and average loss.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PmF1Obu1xIl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    training_pairs = [tensorsFromPair(random.choice(pairs)) for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            \n",
        "            loss_list.append(print_loss_avg)\n",
        "            epoch_list.append(iter)\n",
        "            print('epoch = ',epoch_list[-1],'  loss = ',loss_list[-1])\n",
        "\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yrJp-_A1xIr",
        "colab_type": "text"
      },
      "source": [
        "Plotting results\n",
        "----------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zOTwahz1xIu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def showPlot(loss_list, epoch_list):\n",
        "    plt.plot(epoch_list, loss_list)\n",
        "    plt.xticks(np.arange(0, 75000, 10000)) \n",
        "    plt.yticks(np.arange(0, 5, 0.5)) \n",
        "    plt.savefig(\"test.png\")\n",
        "    plt.show()\n",
        "    plt.close('all')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYK77ZHjk781",
        "colab_type": "text"
      },
      "source": [
        "Making Objects and training\n",
        "---------------------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_k83y18YF5Kd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(output_lang.n_words)\n",
        "print(input_lang.n_words)\n",
        "hidden_size = 256\n",
        "no_of_epoch = 50000\n",
        "# no_hidden_states = 256\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
        "\n",
        "trainIters(encoder1, attn_decoder1, no_of_epoch, print_every=1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1ZdK3GVqvqq",
        "colab_type": "text"
      },
      "source": [
        "Loading and Saving the models\n",
        "--------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVJhARfVEeES",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = F\"/content/gdrive/My Drive/NLPA/EndSemProject/Models/\"\n",
        "datafile = \"pruned_dataset_\"\n",
        "rnn_type = \"BiGRU_\"\n",
        "encoder_model_name = rnn_type+datafile+str(no_of_epoch)+\"_\"+str(hidden_size)+\".encoder\"\n",
        "decoder_model_name = rnn_type+datafile+str(no_of_epoch)+\"_\"+str(hidden_size)+\".attndecoder\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBvQ1CLzhybO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Saving the trained models\n",
        "# device = torch.device('cpu')\n",
        "\n",
        "# torch.save(encoder1.state_dict(), path+encoder_model_name)\n",
        "# torch.save(attn_decoder1.state_dict(), path+decoder_model_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enLs86ImhWEk",
        "colab_type": "code",
        "outputId": "e6678196-1fdd-42d8-bfc7-7fac15389b02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "# Loading the saved models\n",
        "device = torch.device('cpu')\n",
        "model_name = \"BiGRU_pruned_dataset_25000_256.encoder\"\n",
        "encoder_model = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "encoder_model.load_state_dict(torch.load(path+model_name))\n",
        "model_name = \"BiGRU_pruned_dataset_25000_256.attndecoder\"\n",
        "decoder_model = AttnDecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
        "decoder_model.load_state_dict(torch.load(path+model_name))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgjHQTMEiK8i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# showPlot(plot_losses, plot_epoch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxX1ngU71xI0",
        "colab_type": "text"
      },
      "source": [
        "Evaluation\n",
        "=========="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQqS3Uzi1xI1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size*2, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            try:\n",
        "              if topi.item() == UNK_token:\n",
        "                  decoded_words.append('<UNK>')\n",
        "              if topi.item() == EOS_token:\n",
        "                  decoded_words.append('<EOS>')\n",
        "                  break\n",
        "              else:\n",
        "                  decoded_words.append(output_lang.index2word[topi.item()])\n",
        "            except:\n",
        "              continue\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words, decoder_attentions[:di + 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBupSgVq1xI8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(orig_pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXaSkCJ01xJJ",
        "colab_type": "code",
        "outputId": "25069314-28a2-4425-bb08-6fca8fb15be4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 756
        }
      },
      "source": [
        "device = torch.device('cuda')\n",
        "evaluateRandomly(encoder1, attn_decoder1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">  \n",
            "= .\n",
            "< और और है <EOS>\n",
            "\n",
            ">  \n",
            "= .\n",
            "< और और <EOS>\n",
            "\n",
            ">  \n",
            "= .\n",
            "< और और <EOS>\n",
            "\n",
            ">  \n",
            "= .\n",
            "< और और है <EOS>\n",
            "\n",
            ">  \n",
            "= ,\n",
            "< और और <EOS>\n",
            "\n",
            ">  \n",
            "= .\n",
            "< और और <EOS>\n",
            "\n",
            ">  \n",
            "= .\n",
            "< और और <EOS>\n",
            "\n",
            ">  \n",
            "= .\n",
            "< और और <EOS>\n",
            "\n",
            ">  \n",
            "= .\n",
            "< और और <EOS>\n",
            "\n",
            ">  \n",
            "= .\n",
            "< और और <EOS>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYYZqNkuGUJ5",
        "colab_type": "code",
        "outputId": "783edcbb-ecf7-400a-f162-9d8761fc5a92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(epoch_list)\n",
        "print(loss_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n",
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqqadQCjq4vC",
        "colab_type": "text"
      },
      "source": [
        "Evaluation on Test Data\n",
        "------------------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovghTwLqGcbO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction    \n",
        "\n",
        "def calculate_bleu(pred_trg, real_trg):\n",
        "    smoothie = SmoothingFunction().method4\n",
        "    score = sentence_bleu(real_trg, pred_trg, smoothing_function=smoothie)\n",
        "    return score \n",
        "\n",
        "def calculate_Result(encoder, decoder,lcl_pairs, n=50):\n",
        "    device = torch.device('cuda')\n",
        "\n",
        "    result_value_bleu_score = []\n",
        "    \n",
        "    for i in range(n):\n",
        "        pair = random.choice(lcl_pairs)\n",
        "        if debug:\n",
        "          print('>', pair[0])\n",
        "          print('=', pair[1])\n",
        "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        if debug:\n",
        "          print('<', output_sentence)\n",
        "        reference = [pair[1].split()]\n",
        "        if debug:\n",
        "          print('--', reference)\n",
        "        output_words = output_words[:-1]\n",
        "        temp  = []\n",
        "        for ow in output_words:\n",
        "          if ow!='':\n",
        "            temp.append(ow)\n",
        "        output_words = temp\n",
        "        target_predicted = output_words\n",
        "        \n",
        "        if debug:        \n",
        "          print('<<', output_words)\n",
        "        \n",
        "        score = calculate_bleu(target_predicted,reference)\n",
        "        \n",
        "        if debug:\n",
        "          print(\"---Value\",score)\n",
        "        \n",
        "        result_value_bleu_score.append((pair[0],pair[1].split(),target_predicted,score))\n",
        "\n",
        "    return result_value_bleu_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkQao4OFmNop",
        "colab_type": "text"
      },
      "source": [
        "Preparing Testing time language data\n",
        "-------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80GsCzg5Gkax",
        "colab_type": "code",
        "outputId": "ba83c95f-a87b-4f06-9e90-250b149217ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "source": [
        "input_lang, output_lang, pairs = prepareData('eng', 'hi', reverse=False, model=\"test\", filter_sentence=True)\n",
        "print(random.choice(pairs))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Read 291542 sentence pairs\n",
            "Trimmed to 291542 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "eng 105876\n",
            "hi 131312\n",
            "['Swamiji laid the foundation for harmony amongst religions and also harmony between religion and science . ', 'स्वामी जी ने धर्मों के बीच तथा धर्म और विज्ञान के बीच समरसता की आधारशिला रखी । ']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nh5TRotYGnD2",
        "colab_type": "code",
        "outputId": "25fcf5e1-47e9-430c-9ba9-473cc3f337ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "global debug\n",
        "debug = 1\n",
        "device = torch.device('cuda')\n",
        "result_value_bleu_score = calculate_Result(encoder1, attn_decoder1, pairs)\n",
        "result_value_bleu_score_dict = {}\n",
        "result_value_bleu_score_dict['result'] = result_value_bleu_score \n",
        "# torch.save(result_value_bleu_score_dict, train_result_data_path)\n",
        "for item in result_value_bleu_score:\n",
        "  if (float(item[3])>0):\n",
        "    print(\" Source Language \",item[0])\n",
        "    print(\" Input Target\",item[1])\n",
        "    print(\" Output Target\",item[2])\n",
        "    print(\" Score \",item[3])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> The venture was not a success perhaps because of the high prices and the Opera ' s location . \n",
            "= यह प्रयास अंशतः संभवतः इसलिए सफल नहीं हुआ क्योंकि टिकटों की दरें ऊंची थीं और अंशतः इसलिए क्योंकि आपेरा हाउस की स्थिति उपयुक्त नहीं थी । \n",
            "< होगा होगा <EOS>\n",
            "-- [['यह', 'प्रयास', 'अंशतः', 'संभवतः', 'इसलिए', 'सफल', 'नहीं', 'हुआ', 'क्योंकि', 'टिकटों', 'की', 'दरें', 'ऊंची', 'थीं', 'और', 'अंशतः', 'इसलिए', 'क्योंकि', 'आपेरा', 'हाउस', 'की', 'स्थिति', 'उपयुक्त', 'नहीं', 'थी', '।']]\n",
            "<< ['होगा', 'होगा']\n",
            "---Value 0\n",
            "> On 2nd August he left for Lucknow with his elder son for an x - ray examination . \n",
            "= 2 अगस्त को प्रेमचन्द अपने पूत्र धुन्नू के साथ एक्स - रे के लिए लखनऊ गए । \n",
            "< होगा होगा हो <EOS>\n",
            "-- [['2', 'अगस्त', 'को', 'प्रेमचन्द', 'अपने', 'पूत्र', 'धुन्नू', 'के', 'साथ', 'एक्स', '-', 'रे', 'के', 'लिए', 'लखनऊ', 'गए', '।']]\n",
            "<< ['होगा', 'होगा', 'हो']\n",
            "---Value 0\n",
            "> Vaishampayan Telling the Mahabharata to the monks on th eoccasion of the Sarpa Yagna samaroha organised by Janamejaya . \n",
            "= जनमेजय के सर्प यज्ञ समारोह पर वैशम्पायन जी ऋषि मुनियों को महाभारत सुनाते हुए\n",
            "< होगा होगा हो <EOS>\n",
            "-- [['जनमेजय', 'के', 'सर्प', 'यज्ञ', 'समारोह', 'पर', 'वैशम्पायन', 'जी', 'ऋषि', 'मुनियों', 'को', 'महाभारत', 'सुनाते', 'हुए']]\n",
            "<< ['होगा', 'होगा', 'हो']\n",
            "---Value 0\n",
            "> that has not gotten any nectar from the mimic . \n",
            "= जिसे इस नकलची फूल से पराग नहीं मिला है . \n",
            "< होगा होगा हो <EOS>\n",
            "-- [['जिसे', 'इस', 'नकलची', 'फूल', 'से', 'पराग', 'नहीं', 'मिला', 'है', '.']]\n",
            "<< ['होगा', 'होगा', 'हो']\n",
            "---Value 0\n",
            "> I mean , I ' ve got a mouth as dry as the Kalahari Desert . \n",
            "= मतलब यह , कि मेरा मुंह सूख कर कालाहारी रेगिस्तान जैसा हो गया है . \n",
            "< होगा होगा <EOS>\n",
            "-- [['मतलब', 'यह', ',', 'कि', 'मेरा', 'मुंह', 'सूख', 'कर', 'कालाहारी', 'रेगिस्तान', 'जैसा', 'हो', 'गया', 'है', '.']]\n",
            "<< ['होगा', 'होगा']\n",
            "---Value 0\n",
            "> She goes away with stars in her eyes about India , as a result . \n",
            "= और वे अपनी आँखों में भारत की चकाचोंध ले कर वापस जाती थी . \n",
            "< होगा होगा हो <EOS>\n",
            "-- [['और', 'वे', 'अपनी', 'आँखों', 'में', 'भारत', 'की', 'चकाचोंध', 'ले', 'कर', 'वापस', 'जाती', 'थी', '.']]\n",
            "<< ['होगा', 'होगा', 'हो']\n",
            "---Value 0\n",
            "> Sahdev and Guha were sitting almost touching the rope . \n",
            "= संयोग से सहदेव और गुहा बिलकुल सरहद पर बैठे थे । \n",
            "< होगा होगा हो <EOS>\n",
            "-- [['संयोग', 'से', 'सहदेव', 'और', 'गुहा', 'बिलकुल', 'सरहद', 'पर', 'बैठे', 'थे', '।']]\n",
            "<< ['होगा', 'होगा', 'हो']\n",
            "---Value 0\n",
            "> As soon as he sings ' Padmatola ' , ORAL LITERATURE OF OR1SSA : A SURVEY the cobra comes out of the hole , dances moving its broad hood still painted with the sandal mark of Krishna . \n",
            "= जैसे ही वह पद्मतोला गाता है , नाग बांबी से निकल आता है और अपने चौड़े फन को जिस पर अब भी कृष्ण की खड़ाऊं का चिह्न है , घुमाता हुआ नाचने लगता है । \n",
            "< होगा होगा हो <EOS>\n",
            "-- [['जैसे', 'ही', 'वह', 'पद्मतोला', 'गाता', 'है', ',', 'नाग', 'बांबी', 'से', 'निकल', 'आता', 'है', 'और', 'अपने', 'चौड़े', 'फन', 'को', 'जिस', 'पर', 'अब', 'भी', 'कृष्ण', 'की', 'खड़ाऊं', 'का', 'चिह्न', 'है', ',', 'घुमाता', 'हुआ', 'नाचने', 'लगता', 'है', '।']]\n",
            "<< ['होगा', 'होगा', 'हो']\n",
            "---Value 0\n",
            "> Lisbon Portela Airport\n",
            "= पोर्टेला हवाई अड्डा\n",
            "< होगा होगा <EOS>\n",
            "-- [['पोर्टेला', 'हवाई', 'अड्डा']]\n",
            "<< ['होगा', 'होगा']\n",
            "---Value 0\n",
            "> The police officer condoned the rude behavior of the taxi drivers , he just ignored them . \n",
            "= पुलिस अधिकारी ने टेक्सी चालकों के खराब व्यवहार पर ध्यान नहीं दिया , उसने उनकी अनदेखी कर दी । \n",
            "< होगा होगा <EOS>\n",
            "-- [['पुलिस', 'अधिकारी', 'ने', 'टेक्सी', 'चालकों', 'के', 'खराब', 'व्यवहार', 'पर', 'ध्यान', 'नहीं', 'दिया', ',', 'उसने', 'उनकी', 'अनदेखी', 'कर', 'दी', '।']]\n",
            "<< ['होगा', 'होगा']\n",
            "---Value 0\n",
            "> in the whole india sanskrit is studied and in language its given from which indian will become more powerful\n",
            "= पूरे भारत में संस्कृत के अध्ययन - अध्यापन से भारतीय भाषाओं में अधिकाधिक एकरूपता आयेगी जिससे भारतीय एकता बलवती होगी । \n",
            "< होगा होगा <EOS>\n",
            "-- [['पूरे', 'भारत', 'में', 'संस्कृत', 'के', 'अध्ययन', '-', 'अध्यापन', 'से', 'भारतीय', 'भाषाओं', 'में', 'अधिकाधिक', 'एकरूपता', 'आयेगी', 'जिससे', 'भारतीय', 'एकता', 'बलवती', 'होगी', '।']]\n",
            "<< ['होगा', 'होगा']\n",
            "---Value 0\n",
            "> surely your God is One , \n",
            "= कि तुम्हारा पूज्य - प्रभु अकेला है । \n",
            "< होगा होगा हो <EOS>\n",
            "-- [['कि', 'तुम्हारा', 'पूज्य', '-', 'प्रभु', 'अकेला', 'है', '।']]\n",
            "<< ['होगा', 'होगा', 'हो']\n",
            "---Value 0\n",
            "> Bookmark the selected history link\n",
            "= चयनित इतिहास लिंक को पसंद में रखें\n",
            "< होगा होगा <EOS>\n",
            "-- [['चयनित', 'इतिहास', 'लिंक', 'को', 'पसंद', 'में', 'रखें']]\n",
            "<< ['होगा', 'होगा']\n",
            "---Value 0\n",
            "> Select widgets in the workspace\n",
            "= वर्क स्पेस में विजेट चुनें\n",
            "< होगा होगा हो <EOS>\n",
            "-- [['वर्क', 'स्पेस', 'में', 'विजेट', 'चुनें']]\n",
            "<< ['होगा', 'होगा', 'हो']\n",
            "---Value 0\n",
            "> In small firms , the owner or sometimes the key manager generally take charge of the payroll system . \n",
            "= छोटी फर्मों में मालिक या कभी - कभी प्रमुख प्रबंधकर्ता पेरोल प्रणाली का काम संभालता है । \n",
            "< होगा होगा <EOS>\n",
            "-- [['छोटी', 'फर्मों', 'में', 'मालिक', 'या', 'कभी', '-', 'कभी', 'प्रमुख', 'प्रबंधकर्ता', 'पेरोल', 'प्रणाली', 'का', 'काम', 'संभालता', 'है', '।']]\n",
            "<< ['होगा', 'होगा']\n",
            "---Value 0\n",
            "> 25 X 25 cm2 spacing\n",
            "= 25 गुणा 25 वर्ग सेंटीमीटर की जगह\n",
            "< होगा होगा <EOS>\n",
            "-- [['25', 'गुणा', '25', 'वर्ग', 'सेंटीमीटर', 'की', 'जगह']]\n",
            "<< ['होगा', 'होगा']\n",
            "---Value 0\n",
            "> There they will pass a goblet to one another with neither idle talk nor sin , \n",
            "= वहाँ एक दूसरे से शराब का जाम ले लिया करेंगे जिसमें न कोई बेहूदगी है और न गुनाह\n",
            "< होगा होगा हो <EOS>\n",
            "-- [['वहाँ', 'एक', 'दूसरे', 'से', 'शराब', 'का', 'जाम', 'ले', 'लिया', 'करेंगे', 'जिसमें', 'न', 'कोई', 'बेहूदगी', 'है', 'और', 'न', 'गुनाह']]\n",
            "<< ['होगा', 'होगा', 'हो']\n",
            "---Value 0\n",
            "> Break in service\n",
            "= सेवा में व्यवधान\n",
            "< होगा होगा हो <EOS>\n",
            "-- [['सेवा', 'में', 'व्यवधान']]\n",
            "<< ['होगा', 'होगा', 'हो']\n",
            "---Value 0\n",
            "> Who would say , ' Are you indeed of those who believe\n",
            "= और कहा करता था कि क्या तुम भी क़यामत की तसदीक़ करने वालों में हो\n",
            "< होगा होगा <EOS>\n",
            "-- [['और', 'कहा', 'करता', 'था', 'कि', 'क्या', 'तुम', 'भी', 'क़यामत', 'की', 'तसदीक़', 'करने', 'वालों', 'में', 'हो']]\n",
            "<< ['होगा', 'होगा']\n",
            "---Value 0\n",
            "> Solar architecture is generally the architecture of keeping coordination with the climate . “ सौर - स्थापत्य\n",
            "= वस्तुतः जलवायु के साथ सामन्जस्य रखने वाला स्थापत्य है । \n",
            "< होगा होगा हो <EOS>\n",
            "-- [['वस्तुतः', 'जलवायु', 'के', 'साथ', 'सामन्जस्य', 'रखने', 'वाला', 'स्थापत्य', 'है', '।']]\n",
            "<< ['होगा', 'होगा', 'हो']\n",
            "---Value 0\n",
            "> Personnel , Public Grievances and Pensions\n",
            "= कार्मिक , लोक शिकायत और पेंशन\n",
            "< होगा होगा <EOS>\n",
            "-- [['कार्मिक', ',', 'लोक', 'शिकायत', 'और', 'पेंशन']]\n",
            "<< ['होगा', 'होगा']\n",
            "---Value 0\n",
            "> He whom Allah sendeth astray , for him there is no protecting friend after Him . And thou wilt see the evil - doers when they see the doom , they say : Is there any way of return ? \n",
            "= जिस व्यक्ति को अल्लाह गुमराही में डाल दे , तो उसके पश्चात उसे सम्भालनेवाला कोई भी नहीं । तुम ज़ालिमों को देखोगे कि जब वे यातना को देख लेंगे तो कह रहे होंगे , \" क्या लौटने का भी कोई मार्ग है ? \" \n",
            "< होगा होगा <EOS>\n",
            "-- [['जिस', 'व्यक्ति', 'को', 'अल्लाह', 'गुमराही', 'में', 'डाल', 'दे', ',', 'तो', 'उसके', 'पश्चात', 'उसे', 'सम्भालनेवाला', 'कोई', 'भी', 'नहीं', '।', 'तुम', 'ज़ालिमों', 'को', 'देखोगे', 'कि', 'जब', 'वे', 'यातना', 'को', 'देख', 'लेंगे', 'तो', 'कह', 'रहे', 'होंगे', ',', '\"', 'क्या', 'लौटने', 'का', 'भी', 'कोई', 'मार्ग', 'है', '?', '\"']]\n",
            "<< ['होगा', 'होगा']\n",
            "---Value 0\n",
            "> He contested on the platform of the necessity of an alternative leadership . \n",
            "= वैकल्पिक नेतत्व की आवकता के आधार पर ही उन्होनें चुनाव लड़ा था । \n",
            "< होगा होगा हो <EOS>\n",
            "-- [['वैकल्पिक', 'नेतत्व', 'की', 'आवकता', 'के', 'आधार', 'पर', 'ही', 'उन्होनें', 'चुनाव', 'लड़ा', 'था', '।']]\n",
            "<< ['होगा', 'होगा', 'हो']\n",
            "---Value 0\n",
            "> Temporal decisions are habitual\n",
            "= ऐहिक निर्णय प्रवृति दर्शक होते है । \n",
            "< होगा होगा <EOS>\n",
            "-- [['ऐहिक', 'निर्णय', 'प्रवृति', 'दर्शक', 'होते', 'है', '।']]\n",
            "<< ['होगा', 'होगा']\n",
            "---Value 0\n",
            "> And when Our com - mand came to pass , We delivered Hud , together with those who shared his faith , out of special mercy from Us . We delivered them from a woeful chastisement . \n",
            "= और जब हमारा आदेश आ पहुँचा तो हमने हूद और उसके साथ के ईमान लानेवालों को अपनी दयालुता से बचा लिया । और एक कठोर यातना से हमने उन्हें छुटकारा दिया\n",
            "< होगा होगा हो <EOS>\n",
            "-- [['और', 'जब', 'हमारा', 'आदेश', 'आ', 'पहुँचा', 'तो', 'हमने', 'हूद', 'और', 'उसके', 'साथ', 'के', 'ईमान', 'लानेवालों', 'को', 'अपनी', 'दयालुता', 'से', 'बचा', 'लिया', '।', 'और', 'एक', 'कठोर', 'यातना', 'से', 'हमने', 'उन्हें', 'छुटकारा', 'दिया']]\n",
            "<< ['होगा', 'होगा', 'हो']\n",
            "---Value 0\n",
            "> Data Display Debugger\n",
            "= डाटा डिस्प्ले डिबगरName\n",
            "< होगा होगा <EOS>\n",
            "-- [['डाटा', 'डिस्प्ले', 'डिबगरName']]\n",
            "<< ['होगा', 'होगा']\n",
            "---Value 0\n",
            "> Manage available categories\n",
            "= उपलब्ध श्रेणी का प्रबंधन करें\n",
            "< होगा होगा <EOS>\n",
            "-- [['उपलब्ध', 'श्रेणी', 'का', 'प्रबंधन', 'करें']]\n",
            "<< ['होगा', 'होगा']\n",
            "---Value 0\n",
            "> Science and Technology linked on Internet - A long list on internet based materials . \n",
            "= विज्ञान और प्रौद्योगिकी सूत्र इंटरनेट पर - इंटरनेट संसाधनों की एक व्यापक सूची\n",
            "< होगा होगा हो <EOS>\n",
            "-- [['विज्ञान', 'और', 'प्रौद्योगिकी', 'सूत्र', 'इंटरनेट', 'पर', '-', 'इंटरनेट', 'संसाधनों', 'की', 'एक', 'व्यापक', 'सूची']]\n",
            "<< ['होगा', 'होगा', 'हो']\n",
            "---Value 0\n",
            "> Characters in Manik ' s novels of this period are generally remarkable for their courage and intelligence . \n",
            "= माणिक के इस दौर के उपन्यासों के चरित्र सामान्यतः अपने साहस और बुद्धि के कारण अद्वितीय हैं । \n",
            "< होगा होगा <EOS>\n",
            "-- [['माणिक', 'के', 'इस', 'दौर', 'के', 'उपन्यासों', 'के', 'चरित्र', 'सामान्यतः', 'अपने', 'साहस', 'और', 'बुद्धि', 'के', 'कारण', 'अद्वितीय', 'हैं', '।']]\n",
            "<< ['होगा', 'होगा']\n",
            "---Value 0\n",
            "> Those - - - whose hearts , when Allah is mentioned , are filled with awe and who patiently endure that which befalleth them , and these who establish the prayer and of that wherewith We have provided them expend . \n",
            "= ये वे लोग है कि जब अल्लाह को याद किया जाता है तो उनके दिल दहल जाते है और जो मुसीबत उनपर आती है उसपर धैर्य से काम लेते है और नमाज़ को क़ायम करते है , और जो कुछ हमने उन्हें दिया है उसमें से ख़र्च करते है\n",
            "< होगा होगा हो <EOS>\n",
            "-- [['ये', 'वे', 'लोग', 'है', 'कि', 'जब', 'अल्लाह', 'को', 'याद', 'किया', 'जाता', 'है', 'तो', 'उनके', 'दिल', 'दहल', 'जाते', 'है', 'और', 'जो', 'मुसीबत', 'उनपर', 'आती', 'है', 'उसपर', 'धैर्य', 'से', 'काम', 'लेते', 'है', 'और', 'नमाज़', 'को', 'क़ायम', 'करते', 'है', ',', 'और', 'जो', 'कुछ', 'हमने', 'उन्हें', 'दिया', 'है', 'उसमें', 'से', 'ख़र्च', 'करते', 'है']]\n",
            "<< ['होगा', 'होगा', 'हो']\n",
            "---Value 0\n",
            "> When two or more parties stop communicating digitally and resuming the conversation via voice communication over the telephone prototypically this word is used . \n",
            "= जब दो या उससे अधिक दल अपना डिजिटल संचार बीच में रोकें और फिर टेलीफोन पर आवाज संचार के माध्यम से बातचीत मूलरूपतया आरंभ करें तब इस पद का प्रयोग किया जाता है । \n",
            "< होगा होगा हो <EOS>\n",
            "-- [['जब', 'दो', 'या', 'उससे', 'अधिक', 'दल', 'अपना', 'डिजिटल', 'संचार', 'बीच', 'में', 'रोकें', 'और', 'फिर', 'टेलीफोन', 'पर', 'आवाज', 'संचार', 'के', 'माध्यम', 'से', 'बातचीत', 'मूलरूपतया', 'आरंभ', 'करें', 'तब', 'इस', 'पद', 'का', 'प्रयोग', 'किया', 'जाता', 'है', '।']]\n",
            "<< ['होगा', 'होगा', 'हो']\n",
            "---Value 0\n",
            "> Even then , our blood will have twice as much carbon monoxide as oxygen . \n",
            "= ऐसी हालत में भी हमारे खून में कार्बन मोनाक्साइड की मात्रा आक्सीजन की तुलना में दुगुनी होगी । \n",
            "< होगा होगा <EOS>\n",
            "-- [['ऐसी', 'हालत', 'में', 'भी', 'हमारे', 'खून', 'में', 'कार्बन', 'मोनाक्साइड', 'की', 'मात्रा', 'आक्सीजन', 'की', 'तुलना', 'में', 'दुगुनी', 'होगी', '।']]\n",
            "<< ['होगा', 'होगा']\n",
            "---Value 0\n",
            "> The next day , when the elephant again attacked a small party , they shot the poor beast . \n",
            "= दूसरे दिन जब हाथी ने एक छोटी पार्टी पर हमला किया तो उन्होंने उस बेचारे जानवर को गोली मार दी । \n",
            "< होगा होगा हो <EOS>\n",
            "-- [['दूसरे', 'दिन', 'जब', 'हाथी', 'ने', 'एक', 'छोटी', 'पार्टी', 'पर', 'हमला', 'किया', 'तो', 'उन्होंने', 'उस', 'बेचारे', 'जानवर', 'को', 'गोली', 'मार', 'दी', '।']]\n",
            "<< ['होगा', 'होगा', 'हो']\n",
            "---Value 0\n",
            "> Thus began his studies at Roorkee . \n",
            "= इस प्रकार रूड़की में उनका अध्ययन आरंभ हुआ । \n",
            "< होगा होगा हो <EOS>\n",
            "-- [['इस', 'प्रकार', 'रूड़की', 'में', 'उनका', 'अध्ययन', 'आरंभ', 'हुआ', '।']]\n",
            "<< ['होगा', 'होगा', 'हो']\n",
            "---Value 0\n",
            "> He was sure they had come to complain to Anandamoyi and urge her to warn Binoy . \n",
            "= उसने समझा , आनन्दमयी के पास अभियोग लेकर आई है कि विनय को चेतावनी दे दी जाये । \n",
            "< होगा होगा <EOS>\n",
            "-- [['उसने', 'समझा', ',', 'आनन्दमयी', 'के', 'पास', 'अभियोग', 'लेकर', 'आई', 'है', 'कि', 'विनय', 'को', 'चेतावनी', 'दे', 'दी', 'जाये', '।']]\n",
            "<< ['होगा', 'होगा']\n",
            "---Value 0\n",
            "> Much pioneering work was also done of which the benefit was reaped , not only by Visva - Bharati but by many other Universities . \n",
            "= बहुतेरे नवीन कार्य किए गए और उसका लाभ विश्वभारती ही नहीं बल्कि कई अन्य विश्वविद्यालयों को भी मिला । \n",
            "< होगा होगा हो <EOS>\n",
            "-- [['बहुतेरे', 'नवीन', 'कार्य', 'किए', 'गए', 'और', 'उसका', 'लाभ', 'विश्वभारती', 'ही', 'नहीं', 'बल्कि', 'कई', 'अन्य', 'विश्वविद्यालयों', 'को', 'भी', 'मिला', '।']]\n",
            "<< ['होगा', 'होगा', 'हो']\n",
            "---Value 0\n",
            "> These are adorned with golden leaves and precious stones . \n",
            "= इनमें सुवर्ण पर्त भी मढी है तथा बहुमूल्य रत्न जडे़ हैं । \n",
            "< होगा होगा हो <EOS>\n",
            "-- [['इनमें', 'सुवर्ण', 'पर्त', 'भी', 'मढी', 'है', 'तथा', 'बहुमूल्य', 'रत्न', 'जडे़', 'हैं', '।']]\n",
            "<< ['होगा', 'होगा', 'हो']\n",
            "---Value 0\n",
            "> Reclining on green cushions and fair carpets . \n",
            "= वे हरे रेशमी गद्दो और उत्कृष्ट् और असाधारण क़ालीनों पर तकिया लगाए होंगे ; \n",
            "< होगा होगा हो <EOS>\n",
            "-- [['वे', 'हरे', 'रेशमी', 'गद्दो', 'और', 'उत्कृष्ट्', 'और', 'असाधारण', 'क़ालीनों', 'पर', 'तकिया', 'लगाए', 'होंगे', ';']]\n",
            "<< ['होगा', 'होगा', 'हो']\n",
            "---Value 0\n",
            "> Encrypt the new Ubuntu installation for security\n",
            "= सुरक्षा के लिए नए उबुन्टू स्थापना काे एन्क्रिप्ट करें\n",
            "< होगा होगा हो <EOS>\n",
            "-- [['सुरक्षा', 'के', 'लिए', 'नए', 'उबुन्टू', 'स्थापना', 'काे', 'एन्क्रिप्ट', 'करें']]\n",
            "<< ['होगा', 'होगा', 'हो']\n",
            "---Value 0\n",
            "> Kind of any range of life , it always involves some sort of computing today . \n",
            "= जीवन की किसी भी श्रेणी की तरह , इस में कंप्यूटिंग शामिल है । \n",
            "< होगा होगा <EOS>\n",
            "-- [['जीवन', 'की', 'किसी', 'भी', 'श्रेणी', 'की', 'तरह', ',', 'इस', 'में', 'कंप्यूटिंग', 'शामिल', 'है', '।']]\n",
            "<< ['होगा', 'होगा']\n",
            "---Value 0\n",
            "> Assistance is also available for beekeeping activities like production of bee colonies by Bee Breedess , distribution of honey bee colonies , hives and bee keeping equipments . \n",
            "= मधुमक्खी पालन गतिविधियों जैसे बी ब्रीड्स द्वारा मधुमक्खी कालोनियों का उत्पा्दन , मधुमक्खीव कालोनियों का वितरण , छत्ते में शहद एकत्र करना और मधुमक्खी को पालने के औजार के लिए भी सहयोग उपलब्धण है । \n",
            "< होगा होगा <EOS>\n",
            "-- [['मधुमक्खी', 'पालन', 'गतिविधियों', 'जैसे', 'बी', 'ब्रीड्स', 'द्वारा', 'मधुमक्खी', 'कालोनियों', 'का', 'उत्पा्दन', ',', 'मधुमक्खीव', 'कालोनियों', 'का', 'वितरण', ',', 'छत्ते', 'में', 'शहद', 'एकत्र', 'करना', 'और', 'मधुमक्खी', 'को', 'पालने', 'के', 'औजार', 'के', 'लिए', 'भी', 'सहयोग', 'उपलब्धण', 'है', '।']]\n",
            "<< ['होगा', 'होगा']\n",
            "---Value 0\n",
            "> The preferred insect species is guided by vivid colours , markings , hairs and other contrivances directly to the nectary and the stamens or pistil , so that it does not have to waste time and energy searching for the nectary . \n",
            "= चुनी हुई कीट जाति चटख रंगों , चिह्नों , बालों और अन्य प्रयुक़्तियों द्वारा मकरंद कोष और पुंकेसर या स्त्रीकेसर की ओर निर्देशित होती हे ताकि इन्हें मकरंद कोष खोजने के लिए समय और ऊर्जा जाया न करनी पड़े . \n",
            "< होगा होगा हो <EOS>\n",
            "-- [['चुनी', 'हुई', 'कीट', 'जाति', 'चटख', 'रंगों', ',', 'चिह्नों', ',', 'बालों', 'और', 'अन्य', 'प्रयुक़्तियों', 'द्वारा', 'मकरंद', 'कोष', 'और', 'पुंकेसर', 'या', 'स्त्रीकेसर', 'की', 'ओर', 'निर्देशित', 'होती', 'हे', 'ताकि', 'इन्हें', 'मकरंद', 'कोष', 'खोजने', 'के', 'लिए', 'समय', 'और', 'ऊर्जा', 'जाया', 'न', 'करनी', 'पड़े', '.']]\n",
            "<< ['होगा', 'होगा', 'हो']\n",
            "---Value 0\n",
            "> Turn - Over based Fee structure\n",
            "= कुल कारोबार आधारित शुल्क संरचना । \n",
            "< होगा होगा हो <EOS>\n",
            "-- [['कुल', 'कारोबार', 'आधारित', 'शुल्क', 'संरचना', '।']]\n",
            "<< ['होगा', 'होगा', 'हो']\n",
            "---Value 0\n",
            "> The electricity Act also aims to rationalise electricity tariffs , promote efficient and environmentally benign policies , constitute CEA and regulatory commissions , provide for stringent penalties in case of theft of electricity , etc . \n",
            "= विद्युत अधिनियम का लक्ष्यद विद्युत प्रशुल्कक को यौक्तिक बनाना , क्षमता का संवर्धन और पर्यावरण के अनुकूल नीतियां बनाना , सीईए का गठन और विनियामक आयोग का गठन , चोरी आदि के मामले में उल्लं घन करने पर दंड की व्यएवस्थाि करना भी है । \n",
            "< होगा होगा हो <EOS>\n",
            "-- [['विद्युत', 'अधिनियम', 'का', 'लक्ष्यद', 'विद्युत', 'प्रशुल्कक', 'को', 'यौक्तिक', 'बनाना', ',', 'क्षमता', 'का', 'संवर्धन', 'और', 'पर्यावरण', 'के', 'अनुकूल', 'नीतियां', 'बनाना', ',', 'सीईए', 'का', 'गठन', 'और', 'विनियामक', 'आयोग', 'का', 'गठन', ',', 'चोरी', 'आदि', 'के', 'मामले', 'में', 'उल्लं', 'घन', 'करने', 'पर', 'दंड', 'की', 'व्यएवस्थाि', 'करना', 'भी', 'है', '।']]\n",
            "<< ['होगा', 'होगा', 'हो']\n",
            "---Value 0\n",
            "> Those who broke the Word of Allah into several parts . \n",
            "= तो ऐ रसूल तुम्हारे ही परवरदिगार की क़सम\n",
            "< होगा होगा हो <EOS>\n",
            "-- [['तो', 'ऐ', 'रसूल', 'तुम्हारे', 'ही', 'परवरदिगार', 'की', 'क़सम']]\n",
            "<< ['होगा', 'होगा', 'हो']\n",
            "---Value 0\n",
            "> He frankly gave expression to his shortcomings and sincerely repented for them . \n",
            "= उसने स्पष्ट रूप से अपनी त्रुटियों को स्वीकारा है और ईमानदारी से पछतावा भी किया है . \n",
            "< होगा होगा हो <EOS>\n",
            "-- [['उसने', 'स्पष्ट', 'रूप', 'से', 'अपनी', 'त्रुटियों', 'को', 'स्वीकारा', 'है', 'और', 'ईमानदारी', 'से', 'पछतावा', 'भी', 'किया', 'है', '.']]\n",
            "<< ['होगा', 'होगा', 'हो']\n",
            "---Value 0\n",
            "> The young wife with her children will have to lead the rest of her life in seclusion . \n",
            "= इस हालत में युवती विधवा को अपनी बच्चों को संभालते हुए शेष जीवन दुःख दर्द और एकाकीपन में ही बिताना पड़ता है । \n",
            "< होगा होगा हो <EOS>\n",
            "-- [['इस', 'हालत', 'में', 'युवती', 'विधवा', 'को', 'अपनी', 'बच्चों', 'को', 'संभालते', 'हुए', 'शेष', 'जीवन', 'दुःख', 'दर्द', 'और', 'एकाकीपन', 'में', 'ही', 'बिताना', 'पड़ता', 'है', '।']]\n",
            "<< ['होगा', 'होगा', 'हो']\n",
            "---Value 0\n",
            "> Transfer to clean , sterile g1ass bottles and seal with acid proof caps . \n",
            "= इसे अब साफ व स्टेराइल ग्लास बोतल में रख दें और एसिड प्रूफ कैप से सील कर दें । \n",
            "< होगा होगा हो <EOS>\n",
            "-- [['इसे', 'अब', 'साफ', 'व', 'स्टेराइल', 'ग्लास', 'बोतल', 'में', 'रख', 'दें', 'और', 'एसिड', 'प्रूफ', 'कैप', 'से', 'सील', 'कर', 'दें', '।']]\n",
            "<< ['होगा', 'होगा', 'हो']\n",
            "---Value 0\n",
            "> Select right kind of equipments . \n",
            "= सही प्रकार के उपकरणों का चयन करें । \n",
            "< होगा होगा हो <EOS>\n",
            "-- [['सही', 'प्रकार', 'के', 'उपकरणों', 'का', 'चयन', 'करें', '।']]\n",
            "<< ['होगा', 'होगा', 'हो']\n",
            "---Value 0\n",
            "> The envelope was addressed to Thos Jerrom Esq . at Bombay and was posted ' on 4 January 1850 . \n",
            "= यह लिफाफा बंबई के थोस जेरोम नाम के किसी व्यक्ति के नाम 4 जनवरी , 1850 को भेजा गया था । \n",
            "< होगा होगा हो <EOS>\n",
            "-- [['यह', 'लिफाफा', 'बंबई', 'के', 'थोस', 'जेरोम', 'नाम', 'के', 'किसी', 'व्यक्ति', 'के', 'नाम', '4', 'जनवरी', ',', '1850', 'को', 'भेजा', 'गया', 'था', '।']]\n",
            "<< ['होगा', 'होगा', 'हो']\n",
            "---Value 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwAQNTRP_SEz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def showAttention(input_sentence, output_words, attentions):\n",
        "    # Set up figure with colorbar\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    # Set up axes\n",
        "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
        "                       ['<EOS>'], rotation=90)\n",
        "    ax.set_yticklabels([''] + output_words)\n",
        "\n",
        "    # Show label at every tick\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def evaluateAndShowAttention(input_sentence):\n",
        "    output_words, attentions = evaluate(encoder1, attn_decoder1, input_sentence)\n",
        "    print('input =', input_sentence)\n",
        "    print('output =', ' '.join(output_words))\n",
        "    showAttention(input_sentence, output_words, attentions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-EIU7VLpWWI",
        "colab_type": "code",
        "outputId": "ef540250-2c38-47de-a7da-b6f5adc58d21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        }
      },
      "source": [
        "evaluateAndShowAttention(\"So if you have a strategy , use it against Me .\")\n",
        "\n",
        "evaluateAndShowAttention(\"The alliance with Goharbai remained controversial to his dying day .\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input = So if you have a strategy , use it against Me .\n",
            "output = और और है <EOS>\n",
            "input = The alliance with Goharbai remained controversial to his dying day .\n",
            "output = और और है <EOS>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 2324 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 2352 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 2361 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 2376 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 2324 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 2352 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 2361 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 2376 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PyioryNrC35",
        "colab_type": "text"
      },
      "source": [
        "Visualizing Attention\n",
        "---------------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOXlN2P4qLzy",
        "colab_type": "code",
        "outputId": "d52583d2-4845-41bd-a3e1-e6c7f67b10e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "output_words, attentions = evaluate(encoder1, attn_decoder1, \"He seemed to have ultimately attained something he had been hankering after all these years .\")\n",
        "plt.matshow(attentions.numpy())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f29b5f73588>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    }
  ]
}