{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "BiDir_NMT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sY5mSUvX1xGc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "debug = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXUDm2s85ZNZ",
        "colab_type": "code",
        "outputId": "7a7daac4-ff64-4688-f341-468d3a47edae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CXKVJL21xGu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwEmFj511xHA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "UNK_token = 2\n",
        "PAD_token = 3\n",
        "# MAX_LENGTH = 1000\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\", 2:\"UNK\"}\n",
        "        self.n_words = 3  # Count SOS and EOS and UNK\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQMRS-ktqCB6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s\n",
        "\n",
        "def Reverse(lst): \n",
        "    return [ele for ele in reversed(lst)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78wxulOl1xHY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def readLangs(lang1, lang2, reverse=False, model=\"dev\"):\n",
        "    \n",
        "    if model == \"dev\":\n",
        "        source = \"/content/gdrive/My Drive/NLPA/NLA S20 - Assignment 2 Data/enghin/dev.en\"\n",
        "        target = \"/content/gdrive/My Drive/NLPA/NLA S20 - Assignment 2 Data/enghin/dev.hi\"\n",
        "    else if model == \"test\":\n",
        "        source = \"/content/gdrive/My Drive/NLPA/NLA S20 - Assignment 2 Data/enghin/test.en\"\n",
        "        target = \"/content/gdrive/My Drive/NLPA/NLA S20 - Assignment 2 Data/enghin/test.hi\"\n",
        "    else:\n",
        "        source = \"/content/gdrive/My Drive/NLPA/NLA S20 - Assignment 2 Data/enghin/train.en\"\n",
        "        target = \"/content/gdrive/My Drive/NLPA/NLA S20 - Assignment 2 Data/enghin/train.hi\"\n",
        "    \n",
        "    eng_lines = open(source, encoding='utf-8').read().strip().split('\\n')\n",
        "    hin_lines = open(target, encoding='utf-8').read().strip().split('\\n')\n",
        "\n",
        "    lines = []\n",
        "    for i in range(len(eng_lines)):\n",
        "        lines.append(eng_lines[i] + '\\t' + hin_lines[i])\n",
        "\n",
        "    # Split every line into pairs and normalize\n",
        "    # pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "    pairs = [[s for s in l.split('\\t')] for l in lines]\n",
        "    #print(pairs)\n",
        "    # Reverse pairs, make Lang instances\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(lang2)\n",
        "        output_lang = Lang(lang1)\n",
        "    else:\n",
        "        input_lang = Lang(lang1)\n",
        "        output_lang = Lang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37LCeZbCCi9q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LENGTH = 20\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH \n",
        "\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPT1Pzc11xHq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepareData(lang1, lang2, reverse=False, model=\"dev\", filter_sentence=False):\n",
        "    \n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse, model)\n",
        "    #print(input_lang, output_lang, pairs)\n",
        "    if(filter_sentence):\n",
        "        pairs = filterPairs(pairs)\n",
        "    \n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    \n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    \n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    \n",
        "    return input_lang, output_lang, pairs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcRQAZf9DB4O",
        "colab_type": "code",
        "outputId": "caf2850d-87ca-487d-eac2-55e00d9996da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        }
      },
      "source": [
        "input_lang, output_lang, pairs = prepareData('eng', 'hi', reverse=False, model=\"dev\", filter_sentence=True)\n",
        "print(random.choice(pairs))\n",
        "orig_pairs = []\n",
        "for p in pairs:\n",
        "    temp = str(p[0]).split(' ')\n",
        "    temp = Reverse(temp)\n",
        "    listToStr = ' '.join([str(elem) for elem in temp]) \n",
        "#     print(\"listToStr\")\n",
        "#     print(listToStr)\n",
        "    listToStr = str(listToStr)\n",
        "    orig_pairs.append(listToStr+'\\t'+str(p[1]))\n",
        "#     print(orig_pairs[-1])\n",
        "print(\"orig_pairs\")\n",
        "print(orig_pairs[0])\n",
        "print(random.choice(orig_pairs))"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Read 256 sentence pairs\n",
            "Trimmed to 256 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "eng 1117\n",
            "hi 1170\n",
            "['Fever does not go down , diarrhoea does not stop , weight decreases .', 'बुखार नहीं जाता , डायरिया नहीं रूकता , वजन कम हो जाता है ।']\n",
            "orig_pairs\n",
            ". diseases eye from safe be also will you but\tबल्कि आप नेत्ररोगों से भी बचे रहेंगे ।\n",
            ". relations sex unprotected through spreads AIDS\tअसुरक्षित यौन संबंध से एड्स फैलता है ।\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpSD5Bb_1xHz",
        "colab_type": "text"
      },
      "source": [
        "The Encoder\n",
        "-----------\n",
        "\n",
        "The encoder of a seq2seq network is a RNN that outputs some value for\n",
        "every word from the input sentence. For every input word the encoder\n",
        "outputs a vector and a hidden state, and uses the hidden state for the\n",
        "next input word.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDZgT8g_1xH1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers=1, dropout=0.1):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, num_layers, bidirectional=True, dropout=dropout) # batch_first=False,\n",
        "        # self.gru = nn.GRU(self.hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedding = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedding\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "        # packed = pack_padded_sequence(x, lengths, batch_first=True)\n",
        "        # output, final = self.gru(packed)\n",
        "        # output, _ = pad_packed_sequence(output, batch_first=True)\n",
        "\n",
        "        # Manually concatenating the final states for both directions\n",
        "        # fwd_final = final[0:final.size(0):2]\n",
        "        # bwd_final = final[1:final.size(0):2]\n",
        "        # final = torch.cat([fwd_final, bwd_final], dim=2)  # [num_layers, batch, 2*dim]\n",
        "\n",
        "        # return output, final\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(self.num_layers*2, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XajZxxXr1xIF",
        "colab_type": "text"
      },
      "source": [
        "The Attention Decoder\n",
        "---------------------\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2HAV8oS1xIH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, num_layers=1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.num_layers = num_layers\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size*2)\n",
        "\n",
        "        # self.pre_output_layer = nn.Linear(hidden_size + 2*hidden_size + emb_size, hidden_size, bias=False)\n",
        "\n",
        "        # Attention -- general/bilinear\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 4, self.hidden_size)\n",
        "        self.attn_general = nn.Linear(self.max_length, self.max_length)\n",
        "        self.lcl_wa_into_hs = nn.Linear(self.hidden_size*2, self.hidden_size*2)\n",
        "\n",
        "        # self.gru = nn.GRU(emb_size + 2*hidden_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size, self.num_layers, bidirectional=True)\n",
        "        self.out = nn.Linear(self.hidden_size*2, self.output_size)\n",
        "\n",
        "        # self.attn_coverage = nn.Linear(self.max_length, self.hidden_size)\n",
        "        # self.attn_coverage_cat = nn.Linear(self.hidden_size*3, self.max_length)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "       \n",
        "        alphas = self.attn_general(torch.matmul(self.lcl_wa_into_hs(embedded[0]), encoder_outputs.T))\n",
        "        \n",
        "        attn_weights = F.softmax(alphas, dim = 1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(self.num_layers*2, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHlWxIcA1xIM",
        "colab_type": "text"
      },
      "source": [
        "Training\n",
        "========\n",
        "\n",
        "Preparing Training Data\n",
        "-----------------------\n",
        "\n",
        "To train, for each pair we will need an input tensor (indexes of the\n",
        "words in the input sentence) and target tensor (indexes of the words in\n",
        "the target sentence). While creating these vectors we will append the\n",
        "EOS token to both sequences.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUxlLxyS1xIR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] if word in lang.word2index else UNK_token for word in sentence.split(' ')]\n",
        "\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMNj3Uiq1xIX",
        "colab_type": "text"
      },
      "source": [
        "Training the Model\n",
        "------------------\n",
        "\n",
        "To train we run the input sentence through the encoder, and keep track\n",
        "of every output and the latest hidden state. Then the decoder is given\n",
        "the ``<SOS>`` token as its first input, and the last hidden state of the\n",
        "encoder as its first hidden state.\n",
        "\n",
        "\"Teacher forcing\" is the concept of using the real target outputs as\n",
        "each next input, instead of using the decoder's guess as the next input.\n",
        "Using teacher forcing causes it to converge faster but `when the trained\n",
        "network is exploited, it may exhibit\n",
        "instability <http://minds.jacobs-university.de/sites/default/files/uploads/papers/ESNTutorialRev.pdf>`__.\n",
        "\n",
        "You can observe outputs of teacher-forced networks that read with\n",
        "coherent grammar but wander far from the correct translation -\n",
        "intuitively it has learned to represent the output grammar and can \"pick\n",
        "up\" the meaning once the teacher tells it the first few words, but it\n",
        "has not properly learned how to create the sentence from the translation\n",
        "in the first place.\n",
        "\n",
        "Because of the freedom PyTorch's autograd gives us, we can randomly\n",
        "choose to use teacher forcing or not with a simple if statement. Turn\n",
        "``teacher_forcing_ratio`` up to use more of it.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLz4jGbX1xIY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size*2, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "    # decoder_hidden = encoder_outputs\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            # For Paper 2 \n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            \n",
        "            \n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            # For Paper 2 \n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            \n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzusY2u01xId",
        "colab_type": "text"
      },
      "source": [
        "This is a helper function to print time elapsed and estimated time\n",
        "remaining given the current time and progress %.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNovrvZq1xIf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7_F87qsFeOc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_list = []\n",
        "epoch_list = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ib6gNJsH1xIk",
        "colab_type": "text"
      },
      "source": [
        "The whole training process looks like this:\n",
        "\n",
        "-  Start a timer\n",
        "-  Initialize optimizers and criterion\n",
        "-  Create set of training pairs\n",
        "-  Start empty losses array for plotting\n",
        "\n",
        "Then we call ``train`` many times and occasionally print the progress (%\n",
        "of examples, time so far, estimated time) and average loss.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PmF1Obu1xIl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    training_pairs = [tensorsFromPair(random.choice(pairs)) for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            \n",
        "            loss_list.append(print_loss_avg)\n",
        "            epoch_list.append(iter)\n",
        "            print('epoch = ',epoch_list[-1],'  loss = ',loss_list[-1])\n",
        "\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yrJp-_A1xIr",
        "colab_type": "text"
      },
      "source": [
        "Plotting results\n",
        "----------------\n",
        "\n",
        "Plotting is done with matplotlib, using the array of loss values\n",
        "``plot_losses`` saved while training.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zOTwahz1xIu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def showPlot(loss_list, epoch_list):\n",
        "    plt.plot(epoch_list, loss_list)\n",
        "    plt.xticks(np.arange(0, 75000, 10000)) \n",
        "    plt.yticks(np.arange(0, 5, 0.5)) \n",
        "    plt.savefig(\"test.png\")\n",
        "    plt.show()\n",
        "    plt.close('all')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxX1ngU71xI0",
        "colab_type": "text"
      },
      "source": [
        "Evaluation\n",
        "==========\n",
        "\n",
        "Evaluation is mostly the same as training, but there are no targets so\n",
        "we simply feed the decoder's predictions back to itself for each step.\n",
        "Every time it predicts a word we add it to the output string, and if it\n",
        "predicts the EOS token we stop there. We also store the decoder's\n",
        "attention outputs for display later.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQqS3Uzi1xI1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size*2, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            try:\n",
        "              if topi.item() == UNK_token:\n",
        "                  decoded_words.append('<UNK>')\n",
        "              if topi.item() == EOS_token:\n",
        "                  decoded_words.append('<EOS>')\n",
        "                  break\n",
        "              else:\n",
        "                  decoded_words.append(output_lang.index2word[topi.item()])\n",
        "            except:\n",
        "              continue\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words, decoder_attentions[:di + 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhXYW1_81xI6",
        "colab_type": "text"
      },
      "source": [
        "We can evaluate random sentences from the training set and print out the\n",
        "input, target, and output to make some subjective quality judgements:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBupSgVq1xI8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(orig_pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_k83y18YF5Kd",
        "colab_type": "code",
        "outputId": "540ffd80-557c-4588-be95-d48c936f9522",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        }
      },
      "source": [
        "print(output_lang.n_words)\n",
        "print(input_lang.n_words)\n",
        "hidden_size = 256\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
        "\n",
        "trainIters(encoder1, attn_decoder1, 25000, print_every=5000)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1170\n",
            "1117\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch =  5000   loss =  3.259508546816716\n",
            "4m 46s (- 19m 5s) (5000 20%) 3.2595\n",
            "epoch =  10000   loss =  0.17473994010808072\n",
            "9m 49s (- 14m 43s) (10000 40%) 0.1747\n",
            "epoch =  15000   loss =  0.011075238883394704\n",
            "14m 48s (- 9m 52s) (15000 60%) 0.0111\n",
            "epoch =  20000   loss =  0.005814625028227991\n",
            "19m 44s (- 4m 56s) (20000 80%) 0.0058\n",
            "epoch =  25000   loss =  0.004146413236024421\n",
            "24m 40s (- 0m 0s) (25000 100%) 0.0041\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_Nn5BTv1xJC",
        "colab_type": "text"
      },
      "source": [
        "Training and Evaluating\n",
        "=======================\n",
        "\n",
        "With all these helper functions in place (it looks like extra work, but\n",
        "it makes it easier to run multiple experiments) we can actually\n",
        "initialize a network and start training.\n",
        "\n",
        "Remember that the input sentences were heavily filtered. For this small\n",
        "dataset we can use relatively small networks of 256 hidden nodes and a\n",
        "single GRU layer. After about 40 minutes on a MacBook CPU we'll get some\n",
        "reasonable results.\n",
        "\n",
        ".. Note::\n",
        "   If you run this notebook you can train, interrupt the kernel,\n",
        "   evaluate, and continue training later. Comment out the lines where the\n",
        "   encoder and decoder are initialized and run ``trainIters`` again.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oi_fCEhU1xJD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "no_of_epoch = 25000\n",
        "no_hidden_states = 256\n",
        "# model_type = \"AttnDecoder\"\n",
        "# model_name_dec =  model_type+\"_Model_\"+str(no_of_epoch)+\"_\"+str(no_hidden_states)\n",
        "# torch.save(attn_decoder1.state_dict(), model_name_dec)\n",
        "# # device = torch.device('cpu')\n",
        "# decoder_model = AttnDecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
        "# decoder_model.load_state_dict(torch.load(model_name_dec, map_location=device))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVJhARfVEeES",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datafile = \"dev_\"\n",
        "# epochs\n",
        "task = \"BiGRU_\"\n",
        "model_name = task+datafile+str(no_of_epoch)+\"_\"+str(no_hidden_states)+\".encoder\"\n",
        "\n",
        "torch.save(encoder1.state_dict(), model_name)\n",
        "device = torch.device('cpu')\n",
        "# encoder_model = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "# encoder_model.load_state_dict(torch.load(model_name, map_location=device))\n",
        "\n",
        "\n",
        "model_name = task+datafile+str(no_of_epoch)+\"_\"+str(no_hidden_states)+\".attndecoder\"\n",
        "torch.save(attn_decoder1.state_dict(), model_name)\n",
        "device = torch.device('cpu')\n",
        "# decoder_model = DecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
        "# decoder_model.load_state_dict(torch.load(model_name, map_location=device))\n",
        "\n",
        "# showPlot(plot_losses, plot_epoch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXaSkCJ01xJJ",
        "colab_type": "code",
        "outputId": "106980ab-856f-46c6-95a4-991f5578685a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 756
        }
      },
      "source": [
        "device = torch.device('cuda')\n",
        "evaluateRandomly(encoder1, attn_decoder1)"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> .\n",
            "=  \n",
            "< यहाँ से थोड़ा और नीचे बढ़ने पर पहाड़ियों के लिए । <EOS>\n",
            "\n",
            "> .\n",
            "=  \n",
            "< यहाँ से थोड़ा और नीचे बढ़ने पर पहाड़ियों से घिरा गंगामाया मिला । <EOS>\n",
            "\n",
            "> .\n",
            "=  \n",
            "< यहाँ से थोड़ा और नीचे बढ़ने पर पहाड़ियों का इरादा किया । <EOS>\n",
            "\n",
            "> .\n",
            "=  \n",
            "< यहाँ से थोड़ा और नीचे बढ़ने पर पहाड़ियों से घिरा गंगामाया मिला । <EOS>\n",
            "\n",
            "> .\n",
            "=  \n",
            "< यहाँ से थोड़ा और नीचे बढ़ने पर पहाड़ियों के लिए । <EOS>\n",
            "\n",
            "> .\n",
            "=  \n",
            "< यहाँ से थोड़ा और नीचे बढ़ने पर पहाड़ियों से घिरा गंगामाया मिला । <EOS>\n",
            "\n",
            "> .\n",
            "=  \n",
            "< यहाँ से थोड़ा और नीचे बढ़ने पर निश्‍चित ही विजय प्राप्त की जा सकती है । <EOS>\n",
            "\n",
            "> .\n",
            "=  \n",
            "< यहाँ से थोड़ा और नीचे बढ़ने पर पहाड़ियों से घिरा गंगामाया मिला । <EOS>\n",
            "\n",
            "> .\n",
            "=  \n",
            "< यहाँ से थोड़ा और नीचे बढ़ने पर पहाड़ियों के लिए । <EOS>\n",
            "\n",
            "> .\n",
            "=  \n",
            "< यहाँ से थोड़ा और नीचे बढ़ने पर पहाड़ियों के लिए । <EOS>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYYZqNkuGUJ5",
        "colab_type": "code",
        "outputId": "c6fe48c5-e4b0-4933-ddde-41eb6f1e4439",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(epoch_list)\n",
        "print(loss_list)"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[5000, 10000, 15000, 20000, 25000]\n",
            "[3.259508546816716, 0.17473994010808072, 0.011075238883394704, 0.005814625028227991, 0.004146413236024421]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fi-C89yGW7c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "s = \"epoch_list_\"+model_name_dec+'.pkl' \n",
        "with open(s, 'wb') as f:\n",
        "    pickle.dump(epoch_list, f)\n",
        "\n",
        "s = \"loss_list\"+model_name_dec+'.pkl' \n",
        "with open(s, 'wb') as f:\n",
        "    pickle.dump(loss_list, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEkesrjlGYei",
        "colab_type": "code",
        "outputId": "b3810278-f5fc-429c-948e-cc9b958c2353",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "print(\"after loading pickles\")\n",
        "s = \"epoch_list_\"+model_name_dec+'.pkl' \n",
        "with open(s, 'wb') as f:\n",
        "    mynewlist = pickle.load(f)\n",
        "    print(mynewlist)\n",
        "\n",
        "s = \"loss_list\"+model_name_dec+'.pkl' \n",
        "with open(s, 'wb') as f:\n",
        "    mynewlist = pickle.load(f)\n",
        "    print(mynewlist)"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "after loading pickles\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "UnsupportedOperation",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnsupportedOperation\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-98-858a39cdce03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"epoch_list_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mmodel_name_dec\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.pkl'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmynewlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmynewlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnsupportedOperation\u001b[0m: read"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovghTwLqGcbO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction    \n",
        "\n",
        "def calculate_bleu(pred_trg, real_trg):\n",
        "    smoothie = SmoothingFunction().method4\n",
        "    score = sentence_bleu(real_trg, pred_trg, smoothing_function=smoothie)\n",
        "    return score \n",
        "\n",
        "def calculate_Result(encoder, decoder,lcl_pairs, n=50):\n",
        "    device = torch.device('cuda')\n",
        "\n",
        "    result_value_bleu_score = []\n",
        "    \n",
        "    for i in range(n):\n",
        "        pair = random.choice(lcl_pairs)\n",
        "        if debug:\n",
        "          print('>', pair[0])\n",
        "          print('=', pair[1])\n",
        "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        if debug:\n",
        "          print('<', output_sentence)\n",
        "        reference = [pair[1].split()]\n",
        "        if debug:\n",
        "          print('--', reference)\n",
        "        output_words = output_words[:-1]\n",
        "        temp  = []\n",
        "        for ow in output_words:\n",
        "          if ow!='':\n",
        "            temp.append(ow)\n",
        "        output_words = temp\n",
        "        target_predicted = output_words\n",
        "        \n",
        "        if debug:        \n",
        "          print('<<', output_words)\n",
        "        \n",
        "        score = calculate_bleu(target_predicted,reference)\n",
        "        \n",
        "        if debug:\n",
        "          print(\"---Value\",score)\n",
        "        \n",
        "        result_value_bleu_score.append((pair[0],pair[1].split(),target_predicted,score))\n",
        "\n",
        "    return result_value_bleu_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80GsCzg5Gkax",
        "colab_type": "code",
        "outputId": "3916d0bc-669a-4f92-cc36-680a4548990b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "source": [
        "input_lang, output_lang, pairs = prepareData('eng', 'hi', reverse=False, model=\"test\", filter_sentence=True)\n",
        "print(random.choice(pairs))"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Read 30810 sentence pairs\n",
            "Trimmed to 30810 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "eng 23989\n",
            "hi 27143\n",
            "['With this one will get rid of itching and the skin will become soft and radiant as well .', 'अगर आप खाज - खुजली आदि से परेशान हैं तो कच्चे पपीते का दूध निकालकर लगाएँ ।']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nh5TRotYGnD2",
        "colab_type": "code",
        "outputId": "2ba3e402-1ea2-420d-f3e4-c36c74321c2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "global debug\n",
        "debug = 1\n",
        "device = torch.device('cuda')\n",
        "result_value_bleu_score = calculate_Result(encoder1, attn_decoder1, pairs)\n",
        "result_value_bleu_score_dict = {}\n",
        "result_value_bleu_score_dict['result'] = result_value_bleu_score \n",
        "# torch.save(result_value_bleu_score_dict, train_result_data_path)\n",
        "for item in result_value_bleu_score:\n",
        "  if (item[3]>0):\n",
        "    print(\" Source Language \",item[0])\n",
        "    print(\" Input Target\",item[1])\n",
        "    print(\" Output Target\",item[2])\n",
        "    print(\" Score \",item[3])"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> First the coastal journey of Chandra river and then the Bhaga river .\n",
            "= पहले चंद्रनदी का तटीय सफर , फिर भागा नदी का ।\n",
            "< करें आरम्भिक आसानी मसूड़ों टाँके होठों जाता सम्भव <EOS>\n",
            "-- [['पहले', 'चंद्रनदी', 'का', 'तटीय', 'सफर', ',', 'फिर', 'भागा', 'नदी', 'का', '।']]\n",
            "<< ['करें', 'आरम्भिक', 'आसानी', 'मसूड़ों', 'टाँके', 'होठों', 'जाता', 'सम्भव']\n",
            "---Value 0\n",
            "> Yamuna is joining Ganga with a lot of love .\n",
            "= बहुत प्रेम से यमुना गंगा में समा जा रही है ।\n",
            "< होती गये कदम दबाव उद्देश्य बाद प्राथमिक सम्भव <EOS>\n",
            "-- [['बहुत', 'प्रेम', 'से', 'यमुना', 'गंगा', 'में', 'समा', 'जा', 'रही', 'है', '।']]\n",
            "<< ['होती', 'गये', 'कदम', 'दबाव', 'उद्देश्य', 'बाद', 'प्राथमिक', 'सम्भव']\n",
            "---Value 0\n",
            "> On leaving Rangiya there are fields of wheat on both sides of the road .\n",
            "= रंगिया छोड़ते ही सड़क के दोनों ओर धान के खेत हैं ।\n",
            "< होती गये वाला कुष्ठ निकट जीवाणु आँखों कैपस्यूलर छूत हो माइकोबेक्टीरियम वाला लैप्री मुख्यतया क्षमता सीमित सम्भव <EOS>\n",
            "-- [['रंगिया', 'छोड़ते', 'ही', 'सड़क', 'के', 'दोनों', 'ओर', 'धान', 'के', 'खेत', 'हैं', '।']]\n",
            "<< ['होती', 'गये', 'वाला', 'कुष्ठ', 'निकट', 'जीवाणु', 'आँखों', 'कैपस्यूलर', 'छूत', 'हो', 'माइकोबेक्टीरियम', 'वाला', 'लैप्री', 'मुख्यतया', 'क्षमता', 'सीमित', 'सम्भव']\n",
            "---Value 0\n",
            "> The deficiency of iron in body can be because of its absence in adequate amount in food .\n",
            "= शरीर में आयरन की कमी भोजन में इसकी पर्याप्‍त मात्रा में न होने के कारण हो सकती है ।\n",
            "< पैर केवल क्षति जैसे आठ न्यूरोपैथी वाला शल्य-चिकित्सा मैदानी जाता सम्भव <EOS>\n",
            "-- [['शरीर', 'में', 'आयरन', 'की', 'कमी', 'भोजन', 'में', 'इसकी', 'पर्याप्\\u200dत', 'मात्रा', 'में', 'न', 'होने', 'के', 'कारण', 'हो', 'सकती', 'है', '।']]\n",
            "<< ['पैर', 'केवल', 'क्षति', 'जैसे', 'आठ', 'न्यूरोपैथी', 'वाला', 'शल्य-चिकित्सा', 'मैदानी', 'जाता', 'सम्भव']\n",
            "---Value 0\n",
            "> Take a deep breath .\n",
            "= गहरी साँस लें ।\n",
            "< खून जिसकी Small थकान आकार ऑपरेशन ) सम्भव <EOS>\n",
            "-- [['गहरी', 'साँस', 'लें', '।']]\n",
            "<< ['खून', 'जिसकी', 'Small', 'थकान', 'आकार', 'ऑपरेशन', ')', 'सम्भव']\n",
            "---Value 0\n",
            "> When it is empty it returns towards its initial shape .\n",
            "= जब यह खाली है तो अपने शुरुआती आकार की ओर लौटता है ।\n",
            "< निगलने ) शौच कष्ट जीव इन्सुलिन घड़े आपमें सम्भव <EOS>\n",
            "-- [['जब', 'यह', 'खाली', 'है', 'तो', 'अपने', 'शुरुआती', 'आकार', 'की', 'ओर', 'लौटता', 'है', '।']]\n",
            "<< ['निगलने', ')', 'शौच', 'कष्ट', 'जीव', 'इन्सुलिन', 'घड़े', 'आपमें', 'सम्भव']\n",
            "---Value 0\n",
            "> There is restlessness during the occurrence of heart attack .\n",
            "= हार्ट अटैक होने पर घबराहट होती है ।\n",
            "< की भोजन इंद्र चिकन-पौक्स वेरिसिल केवल जाता आना ट्यूमर हो जानी अण्ड फिट सम्भव <EOS>\n",
            "-- [['हार्ट', 'अटैक', 'होने', 'पर', 'घबराहट', 'होती', 'है', '।']]\n",
            "<< ['की', 'भोजन', 'इंद्र', 'चिकन-पौक्स', 'वेरिसिल', 'केवल', 'जाता', 'आना', 'ट्यूमर', 'हो', 'जानी', 'अण्ड', 'फिट', 'सम्भव']\n",
            "---Value 0\n",
            "> for Macau ferry has to be taken from Hong Kong .\n",
            "= हांगकांग से मकाऊ के लिए फेरी लेनी होती है ।\n",
            "< अतिरिक्त दूध आँखों पहुँच केवल यह 1000 कैपस्यूलर बीमारीयों फिट सम्भव <EOS>\n",
            "-- [['हांगकांग', 'से', 'मकाऊ', 'के', 'लिए', 'फेरी', 'लेनी', 'होती', 'है', '।']]\n",
            "<< ['अतिरिक्त', 'दूध', 'आँखों', 'पहुँच', 'केवल', 'यह', '1000', 'कैपस्यूलर', 'बीमारीयों', 'फिट', 'सम्भव']\n",
            "---Value 0\n",
            "> Hammam is joined with hot and cold water channels .\n",
            "= हम्माम गर्म तथा ठण्डे पानी के चैनलों से जुड़ा है ।\n",
            "< होती रहा खिड़कियों फिट S.I.C.S करते हाथ झंझनाहट सुन्नपन वाला ( सूखापन फिट सम्भव <EOS>\n",
            "-- [['हम्माम', 'गर्म', 'तथा', 'ठण्डे', 'पानी', 'के', 'चैनलों', 'से', 'जुड़ा', 'है', '।']]\n",
            "<< ['होती', 'रहा', 'खिड़कियों', 'फिट', 'S.I.C.S', 'करते', 'हाथ', 'झंझनाहट', 'सुन्नपन', 'वाला', '(', 'सूखापन', 'फिट', 'सम्भव']\n",
            "---Value 0\n",
            "> Besides this , there is a glut of museums in Paris .\n",
            "= इसके अलावा पेरिस में म्यूजियमों की भरमार है ।\n",
            "< चाहिए परिवार रोशनी मे इंद्र धनुष समान रंगीन गोले सम्भव <EOS>\n",
            "-- [['इसके', 'अलावा', 'पेरिस', 'में', 'म्यूजियमों', 'की', 'भरमार', 'है', '।']]\n",
            "<< ['चाहिए', 'परिवार', 'रोशनी', 'मे', 'इंद्र', 'धनुष', 'समान', 'रंगीन', 'गोले', 'सम्भव']\n",
            "---Value 0\n",
            "> In reality it is not water it is mirage .\n",
            "= वास्तव में यह पानी नहीं होता , मृगजल होता है ।\n",
            "< कैपस्यूलर होना आँखों खतरा हमेशा ग्रसित आकार वाला ओनिकोमाइसिस शरीर सिर टाँके करानी फिट सम्भव <EOS>\n",
            "-- [['वास्तव', 'में', 'यह', 'पानी', 'नहीं', 'होता', ',', 'मृगजल', 'होता', 'है', '।']]\n",
            "<< ['कैपस्यूलर', 'होना', 'आँखों', 'खतरा', 'हमेशा', 'ग्रसित', 'आकार', 'वाला', 'ओनिकोमाइसिस', 'शरीर', 'सिर', 'टाँके', 'करानी', 'फिट', 'सम्भव']\n",
            "---Value 0\n",
            "> In summers the face of Solang is unique , completely different from that of winters .\n",
            "= गर्मियों में तो सोलंग का रूप ही निराला होता है , सर्दियों से एकदम अलग ।\n",
            "< होती गये कदम दबाव लोगों उभरने केवल क्षमता बारंबारता सम्भव <EOS>\n",
            "-- [['गर्मियों', 'में', 'तो', 'सोलंग', 'का', 'रूप', 'ही', 'निराला', 'होता', 'है', ',', 'सर्दियों', 'से', 'एकदम', 'अलग', '।']]\n",
            "<< ['होती', 'गये', 'कदम', 'दबाव', 'लोगों', 'उभरने', 'केवल', 'क्षमता', 'बारंबारता', 'सम्भव']\n",
            "---Value 0\n",
            "> For this St . Barth is the ideal place .\n",
            "= इसके लिए सेंट बार्थ आदर्श स्थान है ।\n",
            "< लगा ।RD_PUNC चरण देते वाला डॉक्टरी कि Small डॉक्टरी वह सूची जहाँ केवल निर्भर कण फिट सम्भव <EOS>\n",
            "-- [['इसके', 'लिए', 'सेंट', 'बार्थ', 'आदर्श', 'स्थान', 'है', '।']]\n",
            "<< ['लगा', '।RD_PUNC', 'चरण', 'देते', 'वाला', 'डॉक्टरी', 'कि', 'Small', 'डॉक्टरी', 'वह', 'सूची', 'जहाँ', 'केवल', 'निर्भर', 'कण', 'फिट', 'सम्भव']\n",
            "---Value 0\n",
            "> Neroli , rose and frankincense remain beneficial .\n",
            "= नैरोली , रोज , फेकिंसंस भी उपयोगी रहती हैं ।\n",
            "< की जीव भावी इन पौड़ी करेगा प्रबल शल्य-चिकित्सा काला तथा फिट सम्भव <EOS>\n",
            "-- [['नैरोली', ',', 'रोज', ',', 'फेकिंसंस', 'भी', 'उपयोगी', 'रहती', 'हैं', '।']]\n",
            "<< ['की', 'जीव', 'भावी', 'इन', 'पौड़ी', 'करेगा', 'प्रबल', 'शल्य-चिकित्सा', 'काला', 'तथा', 'फिट', 'सम्भव']\n",
            "---Value 0\n",
            "> Anything altered on eyes has direct impact on kidney .\n",
            "= आँखों पर हुआ कुछ भी परिवर्तन सीधा किडनी पर असर करता है ।\n",
            "< वायरस पहले परिवार एडिस बदलते इंद्र रहें घड़े सम्भव <EOS>\n",
            "-- [['आँखों', 'पर', 'हुआ', 'कुछ', 'भी', 'परिवर्तन', 'सीधा', 'किडनी', 'पर', 'असर', 'करता', 'है', '।']]\n",
            "<< ['वायरस', 'पहले', 'परिवार', 'एडिस', 'बदलते', 'इंद्र', 'रहें', 'घड़े', 'सम्भव']\n",
            "---Value 0\n",
            "> Feed only mother 's milk to the child up to six months .\n",
            "= छ: माह तक शिशु को सिर्फ माँ का दूध पिलाएँ ।\n",
            "< दूसरे आँखों आवश्यक आँखों बड़ी जैसे मल शल्य-चिकित्सा जाता सम्भव <EOS>\n",
            "-- [['छ:', 'माह', 'तक', 'शिशु', 'को', 'सिर्फ', 'माँ', 'का', 'दूध', 'पिलाएँ', '।']]\n",
            "<< ['दूसरे', 'आँखों', 'आवश्यक', 'आँखों', 'बड़ी', 'जैसे', 'मल', 'शल्य-चिकित्सा', 'जाता', 'सम्भव']\n",
            "---Value 0\n",
            "> It is called blindness because of diet .\n",
            "= इसको आहार के कारण अंधापन कहते हैं ।\n",
            "< होती गये कदम दबाव उद्देश्य बाद प्राथमिक सम्भव <EOS>\n",
            "-- [['इसको', 'आहार', 'के', 'कारण', 'अंधापन', 'कहते', 'हैं', '।']]\n",
            "<< ['होती', 'गये', 'कदम', 'दबाव', 'उद्देश्य', 'बाद', 'प्राथमिक', 'सम्भव']\n",
            "---Value 0\n",
            "> Do not eat outside food at all .\n",
            "= बाहर का खाना बिल्कुल भी न खाएँ ।\n",
            "< स्तन कि Small थकान आकार ऑपरेशन ) सम्भव <EOS>\n",
            "-- [['बाहर', 'का', 'खाना', 'बिल्कुल', 'भी', 'न', 'खाएँ', '।']]\n",
            "<< ['स्तन', 'कि', 'Small', 'थकान', 'आकार', 'ऑपरेशन', ')', 'सम्भव']\n",
            "---Value 0\n",
            "> The huge Boeing airplane of Air India goes till Air India only .\n",
            "= एयर इंडिया का विशालकाय बोईंग वायुमान रोम तक ही जाता है ।\n",
            "< डॉक्टरी पहले जैसे ट्यूमर परिवार शल्य आमतौर आस-पास ग्रंथियों निकाल प्रवाह फिट सम्भव <EOS>\n",
            "-- [['एयर', 'इंडिया', 'का', 'विशालकाय', 'बोईंग', 'वायुमान', 'रोम', 'तक', 'ही', 'जाता', 'है', '।']]\n",
            "<< ['डॉक्टरी', 'पहले', 'जैसे', 'ट्यूमर', 'परिवार', 'शल्य', 'आमतौर', 'आस-पास', 'ग्रंथियों', 'निकाल', 'प्रवाह', 'फिट', 'सम्भव']\n",
            "---Value 0\n",
            "> While apart from the responsibilities of the house the girls have to give birth to children later on .\n",
            "= जबकि लड़कियों को परिवार की जिम्मेदारियों के अतिरिक्‍त आगे चलकर बच्चे को जन्म देना होता है ।\n",
            "< सुधारने केवल प्रेरित छोटीमाता केवल मैमोग्राम क्षमता निरोध फिट आप <EOS>\n",
            "-- [['जबकि', 'लड़कियों', 'को', 'परिवार', 'की', 'जिम्मेदारियों', 'के', 'अतिरिक्\\u200dत', 'आगे', 'चलकर', 'बच्चे', 'को', 'जन्म', 'देना', 'होता', 'है', '।']]\n",
            "<< ['सुधारने', 'केवल', 'प्रेरित', 'छोटीमाता', 'केवल', 'मैमोग्राम', 'क्षमता', 'निरोध', 'फिट', 'आप']\n",
            "---Value 0\n",
            "> On the occasion of Dussehra the doli of Mother Danteshwar is brought from Dantewada to Jagdalpur .\n",
            "= दशहरे के अवसर में माई दंतेश्‍वरी की डोली को दंतेवाड़ा से जगदलपुर लाया जाता है ।\n",
            "< सकने क्षमता आँखों होती दबाव उद्देश्य बाद प्राथमिक सम्भव <EOS>\n",
            "-- [['दशहरे', 'के', 'अवसर', 'में', 'माई', 'दंतेश्\\u200dवरी', 'की', 'डोली', 'को', 'दंतेवाड़ा', 'से', 'जगदलपुर', 'लाया', 'जाता', 'है', '।']]\n",
            "<< ['सकने', 'क्षमता', 'आँखों', 'होती', 'दबाव', 'उद्देश्य', 'बाद', 'प्राथमिक', 'सम्भव']\n",
            "---Value 0\n",
            "> It plays an important role in regulating the increased level of cholesterol .\n",
            "= यह कोलेस्ट्रॉल की बढ़ी हुई मात्रा को नियंत्रित करने में महत्त्वपूर्ण भूमिका निभाता है ।\n",
            "< तंत्र का असामान्य रूप जीव देता सिर टाँके करानी फिट सम्भव <EOS>\n",
            "-- [['यह', 'कोलेस्ट्रॉल', 'की', 'बढ़ी', 'हुई', 'मात्रा', 'को', 'नियंत्रित', 'करने', 'में', 'महत्त्वपूर्ण', 'भूमिका', 'निभाता', 'है', '।']]\n",
            "<< ['तंत्र', 'का', 'असामान्य', 'रूप', 'जीव', 'देता', 'सिर', 'टाँके', 'करानी', 'फिट', 'सम्भव']\n",
            "---Value 0\n",
            "> In comparison with Vishnu and Shiva the worship of Shakti was more popular in the post-Gupta period Mewar .\n",
            "= उत्तर गुप्तकाल में मेवाड़ क्षेत्र में विष्णु एवं शिव की अपेक्षा शक्ति पूजा अधिक प्रचलित थी ।\n",
            "< होती गये कदम दबाव उद्देश्य बाद प्राथमिक सम्भव <EOS>\n",
            "-- [['उत्तर', 'गुप्तकाल', 'में', 'मेवाड़', 'क्षेत्र', 'में', 'विष्णु', 'एवं', 'शिव', 'की', 'अपेक्षा', 'शक्ति', 'पूजा', 'अधिक', 'प्रचलित', 'थी', '।']]\n",
            "<< ['होती', 'गये', 'कदम', 'दबाव', 'उद्देश्य', 'बाद', 'प्राथमिक', 'सम्भव']\n",
            "---Value 0\n",
            "> HPV disease engulfs around 80 % of the total people who make physical relations .\n",
            "= एचपीवी बीमारी शारीरिक संबंध बनाने वाले लगभग 80% लोगों को घेर लेती है ।\n",
            "< चलते इसका इलाज आरम्भ वाला जा ऐसे फिट सम्भव <EOS>\n",
            "-- [['एचपीवी', 'बीमारी', 'शारीरिक', 'संबंध', 'बनाने', 'वाले', 'लगभग', '80%', 'लोगों', 'को', 'घेर', 'लेती', 'है', '।']]\n",
            "<< ['चलते', 'इसका', 'इलाज', 'आरम्भ', 'वाला', 'जा', 'ऐसे', 'फिट', 'सम्भव']\n",
            "---Value 0\n",
            "> The flow of blood stays good by the use of onion .\n",
            "= प्याज के प्रयोग से रक्त का प्रवाह ठीक रहता है ।\n",
            "< डॉक्टरी पहले जैसे ट्यूमर परिवार शल्य विटामिन-ए व्यक्तियों अवश्य जाता सम्भव <EOS>\n",
            "-- [['प्याज', 'के', 'प्रयोग', 'से', 'रक्त', 'का', 'प्रवाह', 'ठीक', 'रहता', 'है', '।']]\n",
            "<< ['डॉक्टरी', 'पहले', 'जैसे', 'ट्यूमर', 'परिवार', 'शल्य', 'विटामिन-ए', 'व्यक्तियों', 'अवश्य', 'जाता', 'सम्भव']\n",
            "---Value 0\n",
            "> The cancer of reproductive organs in women .\n",
            "= महिलाओं में प्रजनन अंगों का कैंसर ।\n",
            "< की जीव भावी इन पौड़ी करेगा प्रबल शल्य-चिकित्सा मैदानी जाता सम्भव <EOS>\n",
            "-- [['महिलाओं', 'में', 'प्रजनन', 'अंगों', 'का', 'कैंसर', '।']]\n",
            "<< ['की', 'जीव', 'भावी', 'इन', 'पौड़ी', 'करेगा', 'प्रबल', 'शल्य-चिकित्सा', 'मैदानी', 'जाता', 'सम्भव']\n",
            "---Value 0\n",
            "> For the method of dhanurasana lie down on the stomach .\n",
            "= धनुरासन की विधि के लिए पेट के बल लेट जायें ।\n",
            "< डॉक्टरी लगाने जैसे आठ न्यूरोपैथी वाला गुर्दो केवल नेफरोपैथी रेटिनोपैथी ह्दय सम्भव <EOS>\n",
            "-- [['धनुरासन', 'की', 'विधि', 'के', 'लिए', 'पेट', 'के', 'बल', 'लेट', 'जायें', '।']]\n",
            "<< ['डॉक्टरी', 'लगाने', 'जैसे', 'आठ', 'न्यूरोपैथी', 'वाला', 'गुर्दो', 'केवल', 'नेफरोपैथी', 'रेटिनोपैथी', 'ह्दय', 'सम्भव']\n",
            "---Value 0\n",
            "> Situated in the Bilaspur district this lake is also known for water sports .\n",
            "= बिलासपुर जिले में स्थित यह झील जल क्रीड़ाओं के लिए भी जानी जाती है ।\n",
            "< स्तन विटामिन-ए महीन एन्सेफलाइटिस ऑपरेशन लाल सूजन लगवाएँ हेपेटाइटिस-ए एफ.एन.ए.सी. जाता सम्भव <EOS>\n",
            "-- [['बिलासपुर', 'जिले', 'में', 'स्थित', 'यह', 'झील', 'जल', 'क्रीड़ाओं', 'के', 'लिए', 'भी', 'जानी', 'जाती', 'है', '।']]\n",
            "<< ['स्तन', 'विटामिन-ए', 'महीन', 'एन्सेफलाइटिस', 'ऑपरेशन', 'लाल', 'सूजन', 'लगवाएँ', 'हेपेटाइटिस-ए', 'एफ.एन.ए.सी.', 'जाता', 'सम्भव']\n",
            "---Value 0\n",
            "> In these 10 percent of the matters are not able to reach the hospital even .\n",
            "= इसमें 10 प्रतिशत मामले अस्पताल भी ही नहीं पहुँच पाते ।\n",
            "< सारे परिवार एडिस बार प्रत्यारोपण विशिष्ट फुंसियाँ क्षमता ताजी सम्भव <EOS>\n",
            "-- [['इसमें', '10', 'प्रतिशत', 'मामले', 'अस्पताल', 'भी', 'ही', 'नहीं', 'पहुँच', 'पाते', '।']]\n",
            "<< ['सारे', 'परिवार', 'एडिस', 'बार', 'प्रत्यारोपण', 'विशिष्ट', 'फुंसियाँ', 'क्षमता', 'ताजी', 'सम्भव']\n",
            "---Value 0\n",
            "> All the people of the home gather and worship mother Laxmi .\n",
            "= सारे घर के लोग एकत्र होकर माँ लक्ष्मी की आराधना करते हैं ।\n",
            "< होती गये कदम दबाव उद्देश्य बाद जब रह Small खिड़कियों फिट सम्भव <EOS>\n",
            "-- [['सारे', 'घर', 'के', 'लोग', 'एकत्र', 'होकर', 'माँ', 'लक्ष्मी', 'की', 'आराधना', 'करते', 'हैं', '।']]\n",
            "<< ['होती', 'गये', 'कदम', 'दबाव', 'उद्देश्य', 'बाद', 'जब', 'रह', 'Small', 'खिड़कियों', 'फिट', 'सम्भव']\n",
            "---Value 0\n",
            "> Being enchanted by this very beauty tourists enjoy coming to ' Kumarkom ' .\n",
            "= इसी सौंदर्य के वशीभूत हो कर पर्यटक ’ कुमारकोम ’ आना पसंद करते हैं ।\n",
            "< हवा निहित सकते बूंदों कण जाता आँखों तो स्तन कि Small खिड़कियों फिट सम्भव <EOS>\n",
            "-- [['इसी', 'सौंदर्य', 'के', 'वशीभूत', 'हो', 'कर', 'पर्यटक', '’', 'कुमारकोम', '’', 'आना', 'पसंद', 'करते', 'हैं', '।']]\n",
            "<< ['हवा', 'निहित', 'सकते', 'बूंदों', 'कण', 'जाता', 'आँखों', 'तो', 'स्तन', 'कि', 'Small', 'खिड़कियों', 'फिट', 'सम्भव']\n",
            "---Value 0\n",
            "> So much cough accumulates in the lungs that it becomes difficult even to breathe for the patient .\n",
            "= फेफड़ों में इतना कफ जम जाता है कि रोगी के लिए साँस लेना भी कठिन हो जाता है ।\n",
            "< हम ऑपरेशन विकास परिवार रोशनी ओर अग्रसर विटामिन-ए रहे एवं सम्भव <EOS>\n",
            "-- [['फेफड़ों', 'में', 'इतना', 'कफ', 'जम', 'जाता', 'है', 'कि', 'रोगी', 'के', 'लिए', 'साँस', 'लेना', 'भी', 'कठिन', 'हो', 'जाता', 'है', '।']]\n",
            "<< ['हम', 'ऑपरेशन', 'विकास', 'परिवार', 'रोशनी', 'ओर', 'अग्रसर', 'विटामिन-ए', 'रहे', 'एवं', 'सम्भव']\n",
            "---Value 0\n",
            "> It is most necessary to pay attention to children .\n",
            "= सबसे ज्यादा ध्यान बच्चों पर देने की जरूरत है ।\n",
            "< होती बनी रहती मास पेट पेशियाँ वाला हैं फिट जैसे - चुस्त-दुरुस्त वाला बनाने फिट सम्भव <EOS>\n",
            "-- [['सबसे', 'ज्यादा', 'ध्यान', 'बच्चों', 'पर', 'देने', 'की', 'जरूरत', 'है', '।']]\n",
            "<< ['होती', 'बनी', 'रहती', 'मास', 'पेट', 'पेशियाँ', 'वाला', 'हैं', 'फिट', 'जैसे', '-', 'चुस्त-दुरुस्त', 'वाला', 'बनाने', 'फिट', 'सम्भव']\n",
            "---Value 0\n",
            "> After this increase everyday with 50 ml .\n",
            "= इसके बाद रोजाना 50 एम.एल. के हिसाब से बढ़ाएँ ।\n",
            "< डॉक्टरी सेशनों ताजी जाता आँखों सम्बन्ध यदि असाध्य जाता सम्भव <EOS>\n",
            "-- [['इसके', 'बाद', 'रोजाना', '50', 'एम.एल.', 'के', 'हिसाब', 'से', 'बढ़ाएँ', '।']]\n",
            "<< ['डॉक्टरी', 'सेशनों', 'ताजी', 'जाता', 'आँखों', 'सम्बन्ध', 'यदि', 'असाध्य', 'जाता', 'सम्भव']\n",
            "---Value 0\n",
            "> Chettinad -LRB- the extraordinary locality of the Chettiar community -RRB- .\n",
            "= चेट्‍टीनाड ( चेट्‍टीयार समुदाय की अनोखी स्थली )\n",
            "< कि Small सूची जहाँ केवल वह देते तुरंत फिट सम्भव <EOS>\n",
            "-- [['चेट्\\u200dटीनाड', '(', 'चेट्\\u200dटीयार', 'समुदाय', 'की', 'अनोखी', 'स्थली', ')']]\n",
            "<< ['कि', 'Small', 'सूची', 'जहाँ', 'केवल', 'वह', 'देते', 'तुरंत', 'फिट', 'सम्भव']\n",
            "---Value 0\n",
            "> Many temples of Abu Simbel were built during the rule of King Seti itself .\n",
            "= अबू सिंबल के कई मंदिर राजा सेती के शासन काल में ही बने ।\n",
            "< होती गये कदम दबाव उद्देश्य बाद प्राथमिक सम्भव <EOS>\n",
            "-- [['अबू', 'सिंबल', 'के', 'कई', 'मंदिर', 'राजा', 'सेती', 'के', 'शासन', 'काल', 'में', 'ही', 'बने', '।']]\n",
            "<< ['होती', 'गये', 'कदम', 'दबाव', 'उद्देश्य', 'बाद', 'प्राथमिक', 'सम्भव']\n",
            "---Value 0\n",
            "> These pictures could be dynamic or static .\n",
            "= इन चित्रों में चल अथवा स्थिर चित्र हो सकते हैं ।\n",
            "< स्तन शल्य-चिकित्सा अतः Small चिकित्सक जैसे दिखलाएँ ऑपरेशन इसके केवल पत्थरी Surgry विटामिन-ए बने प्रमुख सम्भव <EOS>\n",
            "-- [['इन', 'चित्रों', 'में', 'चल', 'अथवा', 'स्थिर', 'चित्र', 'हो', 'सकते', 'हैं', '।']]\n",
            "<< ['स्तन', 'शल्य-चिकित्सा', 'अतः', 'Small', 'चिकित्सक', 'जैसे', 'दिखलाएँ', 'ऑपरेशन', 'इसके', 'केवल', 'पत्थरी', 'Surgry', 'विटामिन-ए', 'बने', 'प्रमुख', 'सम्भव']\n",
            "---Value 0\n",
            "> Saw ' त्रयम्बक के राशिभूत अट् टहास ' up close .\n",
            "= ’त्रयम्बक के राशिभूत अट्‍टहास’ को पास-पास देखा ।\n",
            "< देखते चाहिए इंद्र हुए साइड ऑपरेशन की का दिखाई इंद्र देना शल्य-चिकित्सा चीजें देख जाता सम्भव <EOS>\n",
            "-- [['’त्रयम्बक', 'के', 'राशिभूत', 'अट्\\u200dटहास’', 'को', 'पास-पास', 'देखा', '।']]\n",
            "<< ['देखते', 'चाहिए', 'इंद्र', 'हुए', 'साइड', 'ऑपरेशन', 'की', 'का', 'दिखाई', 'इंद्र', 'देना', 'शल्य-चिकित्सा', 'चीजें', 'देख', 'जाता', 'सम्भव']\n",
            "---Value 0\n",
            "> Mix about 16 to 22 ml of mustard oil and massage all over body .\n",
            "= डेढ़ - दो तोले सरसों का तेल मिलाकर सारे शरीर पर मल लें ।\n",
            "< देखते केवल प्रेरित छोटीमाता केवल दबाव एक विशेष पानी केवल तथा फिट सम्भव <EOS>\n",
            "-- [['डेढ़', '-', 'दो', 'तोले', 'सरसों', 'का', 'तेल', 'मिलाकर', 'सारे', 'शरीर', 'पर', 'मल', 'लें', '।']]\n",
            "<< ['देखते', 'केवल', 'प्रेरित', 'छोटीमाता', 'केवल', 'दबाव', 'एक', 'विशेष', 'पानी', 'केवल', 'तथा', 'फिट', 'सम्भव']\n",
            "---Value 0\n",
            "> Get a regular medical check up .\n",
            "= मेडिकल चैकअप नियमित करवाएँ ।\n",
            "< गमगीन स्वेत लगातार अंगुलियों तीव्र Small नस्ल नाम कोशिका फिट सम्भव <EOS>\n",
            "-- [['मेडिकल', 'चैकअप', 'नियमित', 'करवाएँ', '।']]\n",
            "<< ['गमगीन', 'स्वेत', 'लगातार', 'अंगुलियों', 'तीव्र', 'Small', 'नस्ल', 'नाम', 'कोशिका', 'फिट', 'सम्भव']\n",
            "---Value 0\n",
            "> From here the rate of auto rickshaw is 50 rupees .\n",
            "= यहाँ से ऑटो रिक्शा का किराया 50 रुपये है ।\n",
            "< अतिरिक्त दूध लिए प्रोत्साहित आँखों करना पर प्रबल शल्य-चिकित्सा मैदानी जाता सम्भव <EOS>\n",
            "-- [['यहाँ', 'से', 'ऑटो', 'रिक्शा', 'का', 'किराया', '50', 'रुपये', 'है', '।']]\n",
            "<< ['अतिरिक्त', 'दूध', 'लिए', 'प्रोत्साहित', 'आँखों', 'करना', 'पर', 'प्रबल', 'शल्य-चिकित्सा', 'मैदानी', 'जाता', 'सम्भव']\n",
            "---Value 0\n",
            "> For example one breast of the women is removed because of the breast cancer .\n",
            "= उदाहरण के लिए कभी-कभी ब्रेस्ट कैंसर की वजह से महिलाओं की एक ब्रेस्ट निकाल दी जाती है ।\n",
            "< कि Small सूची जहाँ केवल वह देते तुरंत फिट सम्भव <EOS>\n",
            "-- [['उदाहरण', 'के', 'लिए', 'कभी-कभी', 'ब्रेस्ट', 'कैंसर', 'की', 'वजह', 'से', 'महिलाओं', 'की', 'एक', 'ब्रेस्ट', 'निकाल', 'दी', 'जाती', 'है', '।']]\n",
            "<< ['कि', 'Small', 'सूची', 'जहाँ', 'केवल', 'वह', 'देते', 'तुरंत', 'फिट', 'सम्भव']\n",
            "---Value 0\n",
            "> A horrible odour begins to come in breathing .\n",
            "= श्‍वास प्रश्‍वास में भयानक दुर्गंध आने लगती है ।\n",
            "< की जीव गाँठ स्त्रियों आम औरतों रक्त जाता आना ट्यूमर हो कुल Small औरतों देते केवल 40 सम्भव <EOS>\n",
            "-- [['श्\\u200dवास', 'प्रश्\\u200dवास', 'में', 'भयानक', 'दुर्गंध', 'आने', 'लगती', 'है', '।']]\n",
            "<< ['की', 'जीव', 'गाँठ', 'स्त्रियों', 'आम', 'औरतों', 'रक्त', 'जाता', 'आना', 'ट्यूमर', 'हो', 'कुल', 'Small', 'औरतों', 'देते', 'केवल', '40', 'सम्भव']\n",
            "---Value 0\n",
            "> Digha has since always remained uncommon .\n",
            "= दीघा हमेशा से असाधारण रहा है ।\n",
            "< पैर केवल क्षति जैसे आठ न्यूरोपैथी वाला गुर्दो केवल नेफरोपैथी रेटिनोपैथी ह्दय सम्भव <EOS>\n",
            "-- [['दीघा', 'हमेशा', 'से', 'असाधारण', 'रहा', 'है', '।']]\n",
            "<< ['पैर', 'केवल', 'क्षति', 'जैसे', 'आठ', 'न्यूरोपैथी', 'वाला', 'गुर्दो', 'केवल', 'नेफरोपैथी', 'रेटिनोपैथी', 'ह्दय', 'सम्भव']\n",
            "---Value 0\n",
            "> If the child drinks milk then make him drink milk regularly .\n",
            "= यदि बच्चा दूध पीता है तो बराबर उसे दूध पिलाती रहें ।\n",
            "< करें आरम्भिक आसानी मसूड़ों टाँके होठों जाता सम्भव <EOS>\n",
            "-- [['यदि', 'बच्चा', 'दूध', 'पीता', 'है', 'तो', 'बराबर', 'उसे', 'दूध', 'पिलाती', 'रहें', '।']]\n",
            "<< ['करें', 'आरम्भिक', 'आसानी', 'मसूड़ों', 'टाँके', 'होठों', 'जाता', 'सम्भव']\n",
            "---Value 0\n",
            "> Ahead of Puraula is Sankhri which is the base camp of Har ki Doon .\n",
            "= पुरौला से आगे है सांखरी जो हर की दून का बेस कैंप है ।\n",
            "< तंत्र का असामान्य रूप जीव देता सिर टाँके करानी फिट सम्भव <EOS>\n",
            "-- [['पुरौला', 'से', 'आगे', 'है', 'सांखरी', 'जो', 'हर', 'की', 'दून', 'का', 'बेस', 'कैंप', 'है', '।']]\n",
            "<< ['तंत्र', 'का', 'असामान्य', 'रूप', 'जीव', 'देता', 'सिर', 'टाँके', 'करानी', 'फिट', 'सम्भव']\n",
            "---Value 0.1477440058878487\n",
            "> Five kilometres ahead of gamshaali is situated Neeti village .\n",
            "= गमशाली में पाँच कि.मी. आगे नीती गाँव स्थित है ।\n",
            "< कैपस्यूलर होना आँखों कम ऑपरेशन ) देता स्वेत पटल सूखा सम्भव <EOS>\n",
            "-- [['गमशाली', 'में', 'पाँच', 'कि.मी.', 'आगे', 'नीती', 'गाँव', 'स्थित', 'है', '।']]\n",
            "<< ['कैपस्यूलर', 'होना', 'आँखों', 'कम', 'ऑपरेशन', ')', 'देता', 'स्वेत', 'पटल', 'सूखा', 'सम्भव']\n",
            "---Value 0\n",
            "> Teeth of children will go bad soon from giving child too much chocolate , biscuit .\n",
            "= अत्यधिक चॉकलेट , बिस्किट देने से बच्चों के दाँत शीघ्र खराब होंगे ।\n",
            "< आदमी जैसे मैमोग्राम केवल प्रवेश काफी वर्जित रह हो खिड़कियों फिट सम्भव <EOS>\n",
            "-- [['अत्यधिक', 'चॉकलेट', ',', 'बिस्किट', 'देने', 'से', 'बच्चों', 'के', 'दाँत', 'शीघ्र', 'खराब', 'होंगे', '।']]\n",
            "<< ['आदमी', 'जैसे', 'मैमोग्राम', 'केवल', 'प्रवेश', 'काफी', 'वर्जित', 'रह', 'हो', 'खिड़कियों', 'फिट', 'सम्भव']\n",
            "---Value 0\n",
            "> In the last financial year the government of Goa earned about 1600 crores of foreign currency through tourism .\n",
            "= पिछले वित्त वर्ष में गोवा सरकार ने तकरीबन 1600 करोड़ रुपए की विदेशी मुद्रा पर्यटन से अर्जित की ।\n",
            "< रोगी उल्टी केवल पहले सके गड्ढे तेल जाता सम्भव <EOS>\n",
            "-- [['पिछले', 'वित्त', 'वर्ष', 'में', 'गोवा', 'सरकार', 'ने', 'तकरीबन', '1600', 'करोड़', 'रुपए', 'की', 'विदेशी', 'मुद्रा', 'पर्यटन', 'से', 'अर्जित', 'की', '।']]\n",
            "<< ['रोगी', 'उल्टी', 'केवल', 'पहले', 'सके', 'गड्ढे', 'तेल', 'जाता', 'सम्भव']\n",
            "---Value 0\n",
            "> According to Anil it 's effect from nowhere was less than Allopathic pain killers .\n",
            "= अनिल के अनुसार इस गोली का असर कहीं से भी एलोपैथिक दर्द निवारकों से कम नहीं था ।\n",
            "< देखते चाहिए इंद्र हुए साइड ऑपरेशन की का दिखाई इंद्र देना शल्य-चिकित्सा चीजें देख जाता सम्भव <EOS>\n",
            "-- [['अनिल', 'के', 'अनुसार', 'इस', 'गोली', 'का', 'असर', 'कहीं', 'से', 'भी', 'एलोपैथिक', 'दर्द', 'निवारकों', 'से', 'कम', 'नहीं', 'था', '।']]\n",
            "<< ['देखते', 'चाहिए', 'इंद्र', 'हुए', 'साइड', 'ऑपरेशन', 'की', 'का', 'दिखाई', 'इंद्र', 'देना', 'शल्य-चिकित्सा', 'चीजें', 'देख', 'जाता', 'सम्भव']\n",
            "---Value 0.16494324790316464\n",
            " Source Language  Ahead of Puraula is Sankhri which is the base camp of Har ki Doon .\n",
            " Input Target ['पुरौला', 'से', 'आगे', 'है', 'सांखरी', 'जो', 'हर', 'की', 'दून', 'का', 'बेस', 'कैंप', 'है', '।']\n",
            " Output Target ['तंत्र', 'का', 'असामान्य', 'रूप', 'जीव', 'देता', 'सिर', 'टाँके', 'करानी', 'फिट', 'सम्भव']\n",
            " Score  0.1477440058878487\n",
            " Source Language  According to Anil it 's effect from nowhere was less than Allopathic pain killers .\n",
            " Input Target ['अनिल', 'के', 'अनुसार', 'इस', 'गोली', 'का', 'असर', 'कहीं', 'से', 'भी', 'एलोपैथिक', 'दर्द', 'निवारकों', 'से', 'कम', 'नहीं', 'था', '।']\n",
            " Output Target ['देखते', 'चाहिए', 'इंद्र', 'हुए', 'साइड', 'ऑपरेशन', 'की', 'का', 'दिखाई', 'इंद्र', 'देना', 'शल्य-चिकित्सा', 'चीजें', 'देख', 'जाता', 'सम्भव']\n",
            " Score  0.16494324790316464\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwAQNTRP_SEz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}